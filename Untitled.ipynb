{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f26d58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "# сделаем симуляцию в среде RL:\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from runstats import *\n",
    "import runstats\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6bc3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# использование дискретного пространства действий, аналогичного документу «Создание рынка с помощью обучения с подкреплением»   (https://arxiv.org/pdf/1804.04216.pdf)\n",
    "actions_num = 21   \n",
    "max_abs_dif = 4\n",
    "max_abs_spread = 20\n",
    "\n",
    "\n",
    "s0 = 100\n",
    "T = 1. # Общее время.\n",
    "sigma = 2.  # Среднеквадратичное отклонение.\n",
    "dt = .005  # Шаг времени.\n",
    "beta = 0.5\n",
    "kappa = beta * 2\n",
    "k = 1.5\n",
    "A = 137.45\n",
    "\n",
    "def spread(beta, sigma, T_t, k):\n",
    "    return beta*sigma**2*(T_t) + 2/beta*np.log(1+beta/k)\n",
    "\n",
    "def r(beta, sigma, T_t, s, q):\n",
    "    return s - q*beta*sigma**2*(T_t)\n",
    "\n",
    "def l(A, k, d):\n",
    "    '''\n",
    "    A : float\n",
    "        согласно Авалленду A = \\lambda/\\alpha, где alpha такая же, как указано выше, \n",
    "        lambda — постоянная частота рыночных ордеров на покупку и продажу.\n",
    "    k : float\n",
    "        согласно Авалленду k = alpha*K, где alpha ~ 1.5, и K таково, что \\delta p ~ Kln(Q) для рыночного ордера размера Q\n",
    "    d : float\n",
    "        согласно Авалленду, d=расстояние до средней цены\n",
    "    \n",
    "    l : float:\n",
    "        согласно Авалленду, l = lambda = интенсивность Пуассона, при которой выполняются приказы нашего агента.\n",
    "    '''\n",
    "    return A*np.exp(-k*d)   \n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, s0, T, dt, sigma, beta, k, A, kappa, seed=0, is_discrete=True):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : float\n",
    "            Начальное значение цены фьючерса/акции.\n",
    "        b : float\n",
    "            Начальное значение «бреча».\n",
    "        T : float\n",
    "            Общее время.\n",
    "        dt : float\n",
    "            Шаг времени.\n",
    "        sigma : float\n",
    "            Волатильность цен.\n",
    "        gamma : float\n",
    "            Фактор дисконта.\n",
    "        k : float\n",
    "            согласно Авалленду k = alpha*K, где alpha ~ 1.5, и K таково, что \\delta p ~ Kln(Q) для рыночного ордера размера Q\n",
    "        A : float\n",
    "            согласно Авалленду A = \\lambda/\\alpha, где alpha такая же, как указано выше, \n",
    "            lambda — постоянная частота рыночных ордеров на покупку и продажу.\n",
    "    \n",
    "        '''\n",
    "        self.s0 = s0\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        self.A = A\n",
    "        self.sqrtdt = np.sqrt(dt)\n",
    "        self.kappa = kappa\n",
    "        self.is_discrete = is_discrete\n",
    "        self.stats = runstats.ExponentialStatistics(decay=0.999)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # пространство наблюдения: s (цена), q, T-t (оставшееся время)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0.0, -math.inf, 0.0]),\n",
    "                                     high=np.array([math.inf, math.inf,T]),\n",
    "                                     dtype=np.float32)\n",
    "        # пространство действия: spread, ds\n",
    "        self.action_space = gym.spaces.Discrete(actions_num)\n",
    "        self.reward_range = (-math.inf,math.inf)\n",
    "        \n",
    "        self.metadata = None # useless field\n",
    "        \n",
    "    def reset(self,seed=0):\n",
    "        self.s = self.s0\n",
    "        self.q = 0.0\n",
    "        self.t = 0.0\n",
    "        self.w = 0.0\n",
    "        self.n = int(T/dt)\n",
    "        self.c_ = 0.0\n",
    "        return np.array((self.s,self.q,self.T))\n",
    "        \n",
    "    def step(self, action):\n",
    "        if self.is_discrete:\n",
    "            despl = (action-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        else:\n",
    "            despl = action\n",
    "        ba_spread = spread(self.beta,self.sigma,self.T-self.t,self.k)\n",
    "\n",
    "        bid = self.s - despl - ba_spread/2\n",
    "        ask = self.s - despl + ba_spread/2\n",
    "                \n",
    "        db = self.s - bid\n",
    "        da = ask - self.s\n",
    "        \n",
    "        lb = l(A, k, db)\n",
    "        la = l(A, k, da)\n",
    "        \n",
    "        dnb = 1 if np.random.uniform() <= lb * self.dt else 0\n",
    "        dna = 1 if np.random.uniform() <= la * self.dt else 0\n",
    "        self.q += dnb - dna\n",
    "\n",
    "        self.c_ += -dnb * bid + dna * ask # заработок\n",
    "\n",
    "        self.s += self.sigma * self.sqrtdt *(1 if np.random.uniform() < 0.5 else -1)\n",
    "\n",
    "        previous_w = self.w\n",
    "        self.w = self.c_ + self.q * self.s\n",
    "                \n",
    "        dw = (self.w - previous_w)\n",
    "        self.stats.push(dw)\n",
    "\n",
    "        reward = dw - self.kappa/2 * (dw - self.stats.mean())**2\n",
    "        \n",
    "        self.t += self.dt\n",
    "            \n",
    "        return np.array((self.s,self.q,self.T-self.t)), reward, self.t >= self.T, {'w':self.w}\n",
    "    \n",
    "env = Environment(s0, T, dt, sigma, beta, k, A,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73653fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузка расширения ноутбука TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3beeb65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель не найдена! Начало обучения...\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/DQN_6\n",
      "Eval num_timesteps=500, episode_reward=-7687.16 +/- 1034.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.69e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.976     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 500       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | -231     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1987     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 900      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-6693.83 +/- 1350.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.69e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.953     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 1000      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-8672.12 +/- 952.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.67e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.929     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 1500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | -219     |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1482     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-8052.21 +/- 1329.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.05e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 2000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-8029.42 +/- 812.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.03e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.881     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 2500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 242      |\n",
      "|    ep_rew_mean      | -268     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1398     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-7660.94 +/- 1192.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.66e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.858     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 3000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-8535.50 +/- 1064.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.54e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.834     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 3500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 244      |\n",
      "|    ep_rew_mean      | -246     |\n",
      "|    exploration_rate | 0.815    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1366     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-7356.55 +/- 1171.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.36e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 4000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-7678.05 +/- 1197.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.68e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.786     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 4500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 245      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1355     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 4900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-8458.45 +/- 833.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.763     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-9327.75 +/- 2838.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -9.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.739     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -216     |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1344     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 5900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-8030.94 +/- 583.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.03e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.715     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 6000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-8437.27 +/- 1724.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.44e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.691     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 6500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -206     |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 6900     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=7000, episode_reward=-7614.03 +/- 721.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.61e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.668     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 7000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-8305.05 +/- 1174.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.31e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.644     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 7500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1334     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 7900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-7912.01 +/- 320.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 8000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-7909.66 +/- 1001.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.596     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 8500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -201     |\n",
      "|    exploration_rate | 0.577    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1330     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 8900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-8553.01 +/- 95.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.55e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.573     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 9000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-8240.86 +/- 1078.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.24e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.549     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 9500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.53     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1326     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 9900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-7788.60 +/- 1173.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.79e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.525     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-7400.68 +/- 850.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.4e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.482    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1311     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 10900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-6894.64 +/- 942.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.89e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.478     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 11000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-7392.34 +/- 757.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.39e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.454     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 11500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1308     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 11900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-8129.03 +/- 792.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.13e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 12000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-7991.46 +/- 752.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.99e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.406     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 12500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.387    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1282     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 12900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-8118.45 +/- 1163.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.12e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.383     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 13000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-8261.59 +/- 1941.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.26e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.359     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 13500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1271     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 13900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=14000, episode_reward=-8085.68 +/- 678.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.335     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 14000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-8950.17 +/- 1536.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.95e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.311     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 14500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -204     |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1268     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 14900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-9227.40 +/- 1807.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -9.23e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.288     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-8542.72 +/- 1586.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.54e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.264     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1266     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 15900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-8042.76 +/- 1151.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 16000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-7302.62 +/- 899.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.197    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1257     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 16900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-7663.11 +/- 568.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.66e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.193     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 17000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-8358.56 +/- 1242.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.36e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.169     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 17500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -199     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1251     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 17900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-7511.66 +/- 411.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.51e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.145     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 18000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-6415.18 +/- 1062.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.42e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.121     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 18500     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -198     |\n",
      "|    exploration_rate | 0.102    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1248     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 18900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-8177.58 +/- 1160.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.18e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0975    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 19000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-7668.45 +/- 499.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.67e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0738    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 19500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.0547   |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1250     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 19900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-7345.14 +/- 657.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.35e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-7692.89 +/- 1586.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.69e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 1254     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 20900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=21000, episode_reward=-8113.69 +/- 1082.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.11e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 21000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-7759.95 +/- 1367.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.76e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 21500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1259     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 21900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-7903.62 +/- 1366.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.9e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-7241.56 +/- 1283.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.24e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 22500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1262     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 22900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-8756.11 +/- 1174.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.76e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 23000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-7923.80 +/- 1434.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.92e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 23500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1265     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 23900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-7631.02 +/- 861.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.63e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 24000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-7472.48 +/- 1488.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.47e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 24500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -198     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1267     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 24900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-6393.87 +/- 773.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.39e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25000     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25500, episode_reward=-8209.51 +/- 1661.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.21e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 1268     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 25900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-7923.84 +/- 769.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.92e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 26000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-7467.41 +/- 1373.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.47e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 26500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 1271     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 26900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-8020.28 +/- 1798.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.02e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 27000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-8945.02 +/- 968.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.95e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 27500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -188     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 1273     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 27900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=28000, episode_reward=-8699.42 +/- 1340.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -8.7e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=-7297.21 +/- 1027.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 1275     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 28900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-8109.52 +/- 874.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.11e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 29000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=-7113.00 +/- 1310.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.11e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 29500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -195     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1276     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 29900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-7331.21 +/- 674.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-7650.94 +/- 962.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.65e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1278     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 30900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-7617.87 +/- 1193.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.62e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 31000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-8383.17 +/- 824.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.38e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 31500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 1279     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 31900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-8699.91 +/- 1260.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -8.7e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-7102.99 +/- 787.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.1e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -196     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 1281     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 32900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-8865.37 +/- 1799.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 33000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-8054.48 +/- 1311.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.05e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 33500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -194     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 1283     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 33900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-8414.10 +/- 1939.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 34000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-8280.85 +/- 1114.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 34500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 1284     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 34900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=-8001.58 +/- 1299.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -8e+03   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-7395.60 +/- 1171.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.4e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 1286     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 35900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-7440.74 +/- 1286.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.44e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 36000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-6897.28 +/- 1053.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.9e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 1288     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 36900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-8707.98 +/- 741.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.71e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 37000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=-7684.47 +/- 973.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.68e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 37500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 1289     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 37900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-8644.09 +/- 837.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.64e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 38000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=-7283.79 +/- 552.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 38500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1290     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 38900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-8149.44 +/- 402.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.15e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 39000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=-7552.79 +/- 1755.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.55e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 39500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 1292     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 39900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-8939.17 +/- 2005.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.94e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=-8246.48 +/- 1735.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.25e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -191     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 1292     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 40900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-7722.87 +/- 387.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.72e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 41000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=-7622.31 +/- 928.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.62e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 41500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 1293     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 41900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=42000, episode_reward=-7627.07 +/- 760.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.63e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 42000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-8267.23 +/- 717.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.27e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 42500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -189     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 1294     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 42900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-8082.58 +/- 578.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.08e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 43000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=-7970.90 +/- 1171.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.97e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 43500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -188     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 1295     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 43900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-8360.15 +/- 926.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.36e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 44000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-7221.29 +/- 536.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.22e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 44500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -189     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 1296     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 44900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-7490.36 +/- 1319.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.49e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-7649.50 +/- 1202.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.65e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -191     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 1297     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 45900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-7740.42 +/- 1061.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 46000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-7324.80 +/- 1215.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.32e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 46500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -185     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 1298     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 46900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-7418.94 +/- 1363.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.42e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 47000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-7409.66 +/- 1264.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 47500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -190     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 47900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-8516.49 +/- 929.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.52e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 48000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-8271.91 +/- 1145.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.27e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 48500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -188     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 1300     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 48900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=49000, episode_reward=-8330.69 +/- 1305.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 49000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-7144.17 +/- 1096.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.14e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 49500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -187     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 49900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-7394.20 +/- 1206.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.39e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-8382.52 +/- 1639.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -8.38e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 9.01      |\n",
      "|    n_updates        | 124       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -423     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 1281     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 50900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.12     |\n",
      "|    n_updates        | 224      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-3527.25 +/- 200.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.53e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 51000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 9.11      |\n",
      "|    n_updates        | 249       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51500, episode_reward=-2507.88 +/- 161.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -2.51e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 51500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 7.55      |\n",
      "|    n_updates        | 374       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -528     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1260     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 51900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.7      |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-437.93 +/- 42.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -438     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.65     |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52500, episode_reward=-12331.86 +/- 1131.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.23e+04 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 52500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 8.16      |\n",
      "|    n_updates        | 624       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -911     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1241     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 52900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.8      |\n",
      "|    n_updates        | 724      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-2914.59 +/- 226.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -2.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 53000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 7.41      |\n",
      "|    n_updates        | 749       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-1507.98 +/- 38.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.51e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 53500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 4.94      |\n",
      "|    n_updates        | 874       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -976     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1223     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 53900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.06     |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-886.30 +/- 55.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -886     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.27     |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=54500, episode_reward=-688.88 +/- 25.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -689     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10       |\n",
      "|    n_updates        | 1124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -996     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1206     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 54900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.59     |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-17.62 +/- 4.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.54     |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55500, episode_reward=-1524.50 +/- 607.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.52e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 55500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.95      |\n",
      "|    n_updates        | 1374      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.02e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 224       |\n",
      "|    fps              | 1190      |\n",
      "|    time_elapsed     | 46        |\n",
      "|    total_timesteps  | 55900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 4.47      |\n",
      "|    n_updates        | 1474      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-1705.15 +/- 304.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.71e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 56000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.65      |\n",
      "|    n_updates        | 1499      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-1933.83 +/- 593.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.93e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 56500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.28      |\n",
      "|    n_updates        | 1624      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.07e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 228       |\n",
      "|    fps              | 1176      |\n",
      "|    time_elapsed     | 48        |\n",
      "|    total_timesteps  | 56900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.84      |\n",
      "|    n_updates        | 1724      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-1322.59 +/- 176.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.32e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 57000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.62      |\n",
      "|    n_updates        | 1749      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=-2041.37 +/- 249.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -2.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 57500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.65      |\n",
      "|    n_updates        | 1874      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.13e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 232       |\n",
      "|    fps              | 1162      |\n",
      "|    time_elapsed     | 49        |\n",
      "|    total_timesteps  | 57900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.28      |\n",
      "|    n_updates        | 1974      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-1735.22 +/- 188.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 58000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.35      |\n",
      "|    n_updates        | 1999      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-479.29 +/- 23.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -479     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.28     |\n",
      "|    n_updates        | 2124     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.14e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 236       |\n",
      "|    fps              | 1149      |\n",
      "|    time_elapsed     | 51        |\n",
      "|    total_timesteps  | 58900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.22      |\n",
      "|    n_updates        | 2224      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-363.91 +/- 20.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -364     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.86     |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=59500, episode_reward=-179.48 +/- 43.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -179     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.958    |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.15e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 240       |\n",
      "|    fps              | 1136      |\n",
      "|    time_elapsed     | 52        |\n",
      "|    total_timesteps  | 59900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 6.3       |\n",
      "|    n_updates        | 2474      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-205.47 +/- 14.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -205     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-27.11 +/- 47.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -27.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.868    |\n",
      "|    n_updates        | 2624     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.14e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 244       |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 54        |\n",
      "|    total_timesteps  | 60900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.22      |\n",
      "|    n_updates        | 2724      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-139.60 +/- 62.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -140     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-23.02 +/- 38.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -23      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 2874     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.15e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 248       |\n",
      "|    fps              | 1114      |\n",
      "|    time_elapsed     | 55        |\n",
      "|    total_timesteps  | 61900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.17      |\n",
      "|    n_updates        | 2974      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=23.82 +/- 7.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.77     |\n",
      "|    n_updates        | 2999     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62500, episode_reward=-28.97 +/- 51.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.9      |\n",
      "|    n_updates        | 3124     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.14e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 252       |\n",
      "|    fps              | 1103      |\n",
      "|    time_elapsed     | 57        |\n",
      "|    total_timesteps  | 62900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.4       |\n",
      "|    n_updates        | 3224      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-52.08 +/- 19.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -52.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.695    |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=-35.25 +/- 25.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -35.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 3374     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.15e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 256       |\n",
      "|    fps              | 1093      |\n",
      "|    time_elapsed     | 58        |\n",
      "|    total_timesteps  | 63900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.67      |\n",
      "|    n_updates        | 3474      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-1541.44 +/- 1463.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -1.54e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 64000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.726     |\n",
      "|    n_updates        | 3499      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-77.44 +/- 19.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -77.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.37     |\n",
      "|    n_updates        | 3624     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 260       |\n",
      "|    fps              | 1083      |\n",
      "|    time_elapsed     | 59        |\n",
      "|    total_timesteps  | 64900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.839     |\n",
      "|    n_updates        | 3724      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-147.84 +/- 35.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -148     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.16     |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-16.92 +/- 15.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -16.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 3874     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 264       |\n",
      "|    fps              | 1074      |\n",
      "|    time_elapsed     | 61        |\n",
      "|    total_timesteps  | 65900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.5       |\n",
      "|    n_updates        | 3974      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-149.66 +/- 62.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -150     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.74     |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=-43.58 +/- 39.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -43.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 4124     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 268       |\n",
      "|    fps              | 1065      |\n",
      "|    time_elapsed     | 62        |\n",
      "|    total_timesteps  | 66900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.718     |\n",
      "|    n_updates        | 4224      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-128.43 +/- 65.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -128     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.843    |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=-28.58 +/- 14.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -28.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.563    |\n",
      "|    n_updates        | 4374     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 272       |\n",
      "|    fps              | 1057      |\n",
      "|    time_elapsed     | 64        |\n",
      "|    total_timesteps  | 67900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.05      |\n",
      "|    n_updates        | 4474      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-63.41 +/- 8.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -63.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=-96.69 +/- 42.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -96.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 4624     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.16e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 276       |\n",
      "|    fps              | 1049      |\n",
      "|    time_elapsed     | 65        |\n",
      "|    total_timesteps  | 68900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.862     |\n",
      "|    n_updates        | 4724      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-55.58 +/- 7.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -55.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.612    |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=-52.99 +/- 7.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -53      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 4874     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.16e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 280       |\n",
      "|    fps              | 1042      |\n",
      "|    time_elapsed     | 67        |\n",
      "|    total_timesteps  | 69900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.834     |\n",
      "|    n_updates        | 4974      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=70000, episode_reward=-101.61 +/- 15.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.21     |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=-54.83 +/- 12.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -54.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.62     |\n",
      "|    n_updates        | 5124     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.15e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 284       |\n",
      "|    fps              | 1034      |\n",
      "|    time_elapsed     | 68        |\n",
      "|    total_timesteps  | 70900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 7.44      |\n",
      "|    n_updates        | 5224      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-51.79 +/- 18.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -51.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 5249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=-81.34 +/- 24.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -81.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.75     |\n",
      "|    n_updates        | 5374     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.15e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 288       |\n",
      "|    fps              | 1027      |\n",
      "|    time_elapsed     | 69        |\n",
      "|    total_timesteps  | 71900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.62      |\n",
      "|    n_updates        | 5474      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=10.07 +/- 18.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.37     |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=-38.05 +/- 19.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -38      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.679    |\n",
      "|    n_updates        | 5624     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.14e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 292       |\n",
      "|    fps              | 1020      |\n",
      "|    time_elapsed     | 71        |\n",
      "|    total_timesteps  | 72900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.01      |\n",
      "|    n_updates        | 5724      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=1.24 +/- 9.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 1.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=-61.35 +/- 15.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -61.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.12     |\n",
      "|    n_updates        | 5874     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.14e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 296       |\n",
      "|    fps              | 1014      |\n",
      "|    time_elapsed     | 72        |\n",
      "|    total_timesteps  | 73900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.55      |\n",
      "|    n_updates        | 5974      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-73.17 +/- 16.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -73.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.62     |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-59.59 +/- 18.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -59.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 6124     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 250       |\n",
      "|    ep_rew_mean      | -1.13e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 300       |\n",
      "|    fps              | 1008      |\n",
      "|    time_elapsed     | 74        |\n",
      "|    total_timesteps  | 74900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.479     |\n",
      "|    n_updates        | 6224      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-71.78 +/- 13.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.838    |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=75500, episode_reward=-73.06 +/- 17.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -73.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.34     |\n",
      "|    n_updates        | 6374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -891     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 1002     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 75900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.552    |\n",
      "|    n_updates        | 6474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-56.98 +/- 12.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -57      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.65     |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-98.73 +/- 27.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -98.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 6624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -781     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 996      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 76900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.681    |\n",
      "|    n_updates        | 6724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-80.06 +/- 17.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -80.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-39.17 +/- 25.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -39.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.73     |\n",
      "|    n_updates        | 6874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -390     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 991      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 77900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.375    |\n",
      "|    n_updates        | 6974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-75.03 +/- 30.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -75      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 6999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=-24.51 +/- 21.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -24.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.32     |\n",
      "|    n_updates        | 7124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -314     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 985      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 78900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 7224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-11.61 +/- 8.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=-28.81 +/- 19.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -28.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.819    |\n",
      "|    n_updates        | 7374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 980      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 79900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-27.75 +/- 30.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -27.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=18.29 +/- 4.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 7624     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -255     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 975      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 80900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 7724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=14.38 +/- 8.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.527    |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=-10.03 +/- 20.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 7874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 971      |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 81900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.75     |\n",
      "|    n_updates        | 7974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-4.88 +/- 15.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.88    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 7999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=19.59 +/- 4.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.861    |\n",
      "|    n_updates        | 8124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -135     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 966      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 82900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.732    |\n",
      "|    n_updates        | 8224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=5.01 +/- 6.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 5.01     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.539    |\n",
      "|    n_updates        | 8249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=21.96 +/- 12.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.523    |\n",
      "|    n_updates        | 8374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -114     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 962      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 83900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 8474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=13.16 +/- 7.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.83     |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=18.55 +/- 4.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.65     |\n",
      "|    n_updates        | 8624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -103     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 957      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 84900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.95     |\n",
      "|    n_updates        | 8724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-0.36 +/- 15.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.363   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=26.62 +/- 3.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 8874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 953      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 85900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.968    |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=86000, episode_reward=-30.98 +/- 22.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -31      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.737    |\n",
      "|    n_updates        | 8999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=24.66 +/- 2.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.33     |\n",
      "|    n_updates        | 9124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -91.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 86900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.83     |\n",
      "|    n_updates        | 9224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=20.22 +/- 4.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.575    |\n",
      "|    n_updates        | 9249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=25.89 +/- 5.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 9374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -83.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 945      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 87900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 9474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=16.10 +/- 4.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.8      |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=25.06 +/- 3.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 9624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -67.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 941      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 88900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.984    |\n",
      "|    n_updates        | 9724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=15.82 +/- 14.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.682    |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=29.62 +/- 5.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 29.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 9874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -32.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 938      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 89900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=26.26 +/- 4.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.14     |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=16.04 +/- 2.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.532    |\n",
      "|    n_updates        | 10124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -27.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 934      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 90900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 10224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=26.05 +/- 2.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 10249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=91500, episode_reward=-119.14 +/- 18.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.479    |\n",
      "|    n_updates        | 10374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -22      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 931      |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 91900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.934    |\n",
      "|    n_updates        | 10474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=18.99 +/- 3.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.814    |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=26.23 +/- 3.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.34     |\n",
      "|    n_updates        | 10624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -18.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 927      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 92900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.466    |\n",
      "|    n_updates        | 10724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=26.28 +/- 4.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.572    |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=27.15 +/- 4.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.951    |\n",
      "|    n_updates        | 10874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -15.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 924      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 93900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.15     |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=22.61 +/- 3.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.43     |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=18.95 +/- 8.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 11124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -10.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 921      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 94900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.896    |\n",
      "|    n_updates        | 11224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=20.03 +/- 2.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.916    |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=28.72 +/- 4.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.746    |\n",
      "|    n_updates        | 11374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -6.54    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 918      |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 95900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.728    |\n",
      "|    n_updates        | 11474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=5.28 +/- 2.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 5.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.75     |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=7.79 +/- 6.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.39     |\n",
      "|    n_updates        | 11624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -4.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 915      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 96900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.468    |\n",
      "|    n_updates        | 11724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=27.99 +/- 2.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.24     |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=30.34 +/- 4.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.66     |\n",
      "|    n_updates        | 11874    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -1.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 912      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 97900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.572    |\n",
      "|    n_updates        | 11974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=25.95 +/- 2.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.901    |\n",
      "|    n_updates        | 11999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=21.17 +/- 5.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.97     |\n",
      "|    n_updates        | 12124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 1.52     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 909      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 98900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 12224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=28.93 +/- 2.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.954    |\n",
      "|    n_updates        | 12249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=7.44 +/- 6.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.332    |\n",
      "|    n_updates        | 12374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 4.84     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 906      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 99900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=26.88 +/- 2.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=24.24 +/- 3.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 12624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 7.58     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 904      |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 100900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 12724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=22.64 +/- 4.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.17     |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=25.67 +/- 3.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.79     |\n",
      "|    n_updates        | 12874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.36     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 901      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 101900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.62     |\n",
      "|    n_updates        | 12974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=102000, episode_reward=3.86 +/- 3.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 3.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.581    |\n",
      "|    n_updates        | 12999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=15.71 +/- 4.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 13124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 898      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 102900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.548    |\n",
      "|    n_updates        | 13224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=13.03 +/- 7.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.738    |\n",
      "|    n_updates        | 13249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=24.73 +/- 1.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 13374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 896      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 103900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 13474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=16.38 +/- 10.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=30.75 +/- 5.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 13624    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 104900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.829    |\n",
      "|    n_updates        | 13724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=21.67 +/- 2.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.934    |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=28.05 +/- 2.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.531    |\n",
      "|    n_updates        | 13874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 891      |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 105900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 13974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=26.37 +/- 4.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.723    |\n",
      "|    n_updates        | 13999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=18.68 +/- 5.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.73     |\n",
      "|    n_updates        | 14124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 888      |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 106900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23     |\n",
      "|    n_updates        | 14224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=26.18 +/- 4.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.849    |\n",
      "|    n_updates        | 14249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=107500, episode_reward=12.41 +/- 3.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.657    |\n",
      "|    n_updates        | 14374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 886      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 107900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.335    |\n",
      "|    n_updates        | 14474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=16.63 +/- 8.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.82     |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=20.94 +/- 7.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.803    |\n",
      "|    n_updates        | 14624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 884      |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 108900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.459    |\n",
      "|    n_updates        | 14724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=3.65 +/- 20.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 3.65     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.63     |\n",
      "|    n_updates        | 14749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=26.15 +/- 4.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.237    |\n",
      "|    n_updates        | 14874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 882      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 109900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.34     |\n",
      "|    n_updates        | 14974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=27.28 +/- 2.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.647    |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=20.21 +/- 9.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.691    |\n",
      "|    n_updates        | 15124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 880      |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 110900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 15224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=6.26 +/- 7.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.26     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 15249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=22.96 +/- 3.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 15374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 878      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 111900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.495    |\n",
      "|    n_updates        | 15474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=30.32 +/- 4.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.53     |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=18.32 +/- 5.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.514    |\n",
      "|    n_updates        | 15624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 16.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 876      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 112900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.24     |\n",
      "|    n_updates        | 15724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=25.45 +/- 3.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.35     |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=20.64 +/- 7.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.491    |\n",
      "|    n_updates        | 15874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 874      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 113900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 15974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=23.60 +/- 2.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 15999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=27.45 +/- 2.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.507    |\n",
      "|    n_updates        | 16124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 872      |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 114900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.527    |\n",
      "|    n_updates        | 16224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=22.48 +/- 6.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.872    |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=20.76 +/- 5.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.5      |\n",
      "|    n_updates        | 16374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 870      |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 115900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.5      |\n",
      "|    n_updates        | 16474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=17.35 +/- 7.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=28.07 +/- 1.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 16624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 868      |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 116900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 16724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=27.03 +/- 4.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.26     |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=15.35 +/- 4.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.246    |\n",
      "|    n_updates        | 16874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 866      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 117900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 16974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=118000, episode_reward=25.41 +/- 4.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.619    |\n",
      "|    n_updates        | 16999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=20.80 +/- 4.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 17124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 864      |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 118900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.481    |\n",
      "|    n_updates        | 17224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=20.44 +/- 5.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.49     |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=8.54 +/- 7.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.628    |\n",
      "|    n_updates        | 17374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 863      |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 119900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 17474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=9.51 +/- 5.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.254    |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=18.68 +/- 2.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.756    |\n",
      "|    n_updates        | 17624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 861      |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 120900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.886    |\n",
      "|    n_updates        | 17724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=26.33 +/- 1.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.484    |\n",
      "|    n_updates        | 17749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=22.10 +/- 5.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.614    |\n",
      "|    n_updates        | 17874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 859      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 121900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.681    |\n",
      "|    n_updates        | 17974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=27.72 +/- 5.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.824    |\n",
      "|    n_updates        | 17999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=-76.02 +/- 34.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -76      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 18124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 857      |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 122900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.651    |\n",
      "|    n_updates        | 18224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=24.30 +/- 5.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.261    |\n",
      "|    n_updates        | 18249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=123500, episode_reward=24.33 +/- 3.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.808    |\n",
      "|    n_updates        | 18374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 856      |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 123900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 18474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=24.78 +/- 3.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.78     |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=21.90 +/- 5.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 18624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 854      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 124900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.551    |\n",
      "|    n_updates        | 18724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=27.56 +/- 2.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.693    |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=24.66 +/- 4.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.783    |\n",
      "|    n_updates        | 18874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 853      |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 125900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.03     |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=27.77 +/- 2.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.384    |\n",
      "|    n_updates        | 18999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=23.60 +/- 2.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.6      |\n",
      "|    n_updates        | 19124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 851      |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 126900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 19224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=21.28 +/- 1.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.354    |\n",
      "|    n_updates        | 19249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=19.92 +/- 2.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.467    |\n",
      "|    n_updates        | 19374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 850      |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 127900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 19474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=17.07 +/- 10.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.149    |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=9.52 +/- 5.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.52     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 19624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 128900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.514    |\n",
      "|    n_updates        | 19724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=18.41 +/- 2.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.81     |\n",
      "|    n_updates        | 19749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=21.86 +/- 4.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.506    |\n",
      "|    n_updates        | 19874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 847      |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 129900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 19974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-11.58 +/- 36.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=24.48 +/- 4.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.315    |\n",
      "|    n_updates        | 20124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 846      |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 130900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.335    |\n",
      "|    n_updates        | 20224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=28.17 +/- 3.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 20249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=19.98 +/- 5.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 20374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 844      |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 131900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.735    |\n",
      "|    n_updates        | 20474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=18.88 +/- 5.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=24.79 +/- 2.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.58     |\n",
      "|    n_updates        | 20624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 843      |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 132900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 20724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=11.12 +/- 9.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.415    |\n",
      "|    n_updates        | 20749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=-159.12 +/- 22.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -159     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.452    |\n",
      "|    n_updates        | 20874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 841      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 133900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 20974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=134000, episode_reward=28.88 +/- 7.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.27     |\n",
      "|    n_updates        | 20999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=16.57 +/- 5.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.54     |\n",
      "|    n_updates        | 21124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 840      |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 134900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 21224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=24.22 +/- 2.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=27.79 +/- 4.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 21374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 839      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 135900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.72     |\n",
      "|    n_updates        | 21474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=12.82 +/- 7.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.63     |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=25.85 +/- 1.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.978    |\n",
      "|    n_updates        | 21624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 838      |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 136900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.531    |\n",
      "|    n_updates        | 21724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=23.19 +/- 3.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.292    |\n",
      "|    n_updates        | 21749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=26.06 +/- 10.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.885    |\n",
      "|    n_updates        | 21874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 836      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 137900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.86     |\n",
      "|    n_updates        | 21974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=23.70 +/- 4.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 21999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=20.62 +/- 4.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.88     |\n",
      "|    n_updates        | 22124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 835      |\n",
      "|    time_elapsed     | 166      |\n",
      "|    total_timesteps  | 138900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 22224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=19.93 +/- 5.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.61     |\n",
      "|    n_updates        | 22249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=139500, episode_reward=23.83 +/- 9.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.99     |\n",
      "|    n_updates        | 22374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 834      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 139900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.11     |\n",
      "|    n_updates        | 22474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-2.82 +/- 6.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.613    |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=20.35 +/- 6.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.827    |\n",
      "|    n_updates        | 22624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 833      |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total_timesteps  | 140900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 22724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=25.95 +/- 2.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 22749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=20.17 +/- 3.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.617    |\n",
      "|    n_updates        | 22874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 832      |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 141900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.732    |\n",
      "|    n_updates        | 22974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=16.99 +/- 4.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.764    |\n",
      "|    n_updates        | 22999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=24.12 +/- 2.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.27     |\n",
      "|    n_updates        | 23124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 142900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.379    |\n",
      "|    n_updates        | 23224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=26.11 +/- 5.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 23249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=19.57 +/- 0.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 23374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 13.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 829      |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 143900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.774    |\n",
      "|    n_updates        | 23474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=17.21 +/- 9.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=25.60 +/- 5.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.48     |\n",
      "|    n_updates        | 23624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 828      |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total_timesteps  | 144900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 23724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=22.99 +/- 4.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.521    |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=20.46 +/- 4.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.576    |\n",
      "|    n_updates        | 23874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 827      |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 145900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.71     |\n",
      "|    n_updates        | 23974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=16.05 +/- 10.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.407    |\n",
      "|    n_updates        | 23999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=23.76 +/- 2.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.292    |\n",
      "|    n_updates        | 24124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 826      |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total_timesteps  | 146900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 24224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=12.77 +/- 4.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.76     |\n",
      "|    n_updates        | 24249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=19.58 +/- 7.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.448    |\n",
      "|    n_updates        | 24374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 825      |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total_timesteps  | 147900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 24474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=21.41 +/- 5.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.471    |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=23.45 +/- 9.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.291    |\n",
      "|    n_updates        | 24624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 824      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 148900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.766    |\n",
      "|    n_updates        | 24724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=20.26 +/- 5.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.564    |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=14.23 +/- 8.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 24874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 823      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 149900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.764    |\n",
      "|    n_updates        | 24974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=150000, episode_reward=22.40 +/- 4.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.257    |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=19.70 +/- 6.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 25124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 822      |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 150900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.451    |\n",
      "|    n_updates        | 25224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=17.30 +/- 4.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 25249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=5.71 +/- 3.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 5.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 25374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 821      |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total_timesteps  | 151900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 25474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-95.18 +/- 29.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -95.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 25499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=-67.56 +/- 28.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.87     |\n",
      "|    n_updates        | 25624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.91     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 820      |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 152900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.255    |\n",
      "|    n_updates        | 25724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=20.09 +/- 5.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 25749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=7.75 +/- 4.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.374    |\n",
      "|    n_updates        | 25874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.83     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 819      |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total_timesteps  | 153900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.709    |\n",
      "|    n_updates        | 25974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=21.33 +/- 5.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.288    |\n",
      "|    n_updates        | 25999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=13.34 +/- 7.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 26124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.66     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 818      |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total_timesteps  | 154900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.46     |\n",
      "|    n_updates        | 26224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=21.16 +/- 3.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.7      |\n",
      "|    n_updates        | 26249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=155500, episode_reward=10.51 +/- 3.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 26374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 8.94     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 817      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 155900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.713    |\n",
      "|    n_updates        | 26474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=15.72 +/- 7.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 26499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=16.95 +/- 9.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 26624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 8.56     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total_timesteps  | 156900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.483    |\n",
      "|    n_updates        | 26724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=10.80 +/- 8.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 26749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=19.54 +/- 6.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.543    |\n",
      "|    n_updates        | 26874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.75     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 815      |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 157900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.547    |\n",
      "|    n_updates        | 26974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=19.01 +/- 6.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 26999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=18.67 +/- 6.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 27124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 814      |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 158900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.35     |\n",
      "|    n_updates        | 27224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=11.57 +/- 4.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.667    |\n",
      "|    n_updates        | 27249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=23.00 +/- 4.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.459    |\n",
      "|    n_updates        | 27374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 814      |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 159900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.889    |\n",
      "|    n_updates        | 27474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=21.42 +/- 4.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.251    |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=25.35 +/- 4.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.477    |\n",
      "|    n_updates        | 27624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 813      |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total_timesteps  | 160900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 27724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=25.75 +/- 3.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=18.67 +/- 4.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 27874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 161900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.725    |\n",
      "|    n_updates        | 27974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=21.96 +/- 9.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 27999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=15.02 +/- 3.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 28124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 811      |\n",
      "|    time_elapsed     | 200      |\n",
      "|    total_timesteps  | 162900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 28224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=27.74 +/- 5.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.44     |\n",
      "|    n_updates        | 28249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=20.53 +/- 6.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 28374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 810      |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total_timesteps  | 163900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 28474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=17.05 +/- 5.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.462    |\n",
      "|    n_updates        | 28499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=15.67 +/- 1.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 28624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 164900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.716    |\n",
      "|    n_updates        | 28724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=7.49 +/- 4.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.49     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.406    |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=8.02 +/- 6.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.02     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 28874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 165900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 28974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=166000, episode_reward=11.29 +/- 2.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.51     |\n",
      "|    n_updates        | 28999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=13.05 +/- 5.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 29124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 808      |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 166900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.266    |\n",
      "|    n_updates        | 29224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=17.50 +/- 5.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.398    |\n",
      "|    n_updates        | 29249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=23.25 +/- 3.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 29374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 167900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 29474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=20.67 +/- 4.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.699    |\n",
      "|    n_updates        | 29499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=6.70 +/- 6.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.51     |\n",
      "|    n_updates        | 29624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 806      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 168900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 29724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=22.26 +/- 7.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.98     |\n",
      "|    n_updates        | 29749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=10.93 +/- 10.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 29874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 805      |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 169900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.37     |\n",
      "|    n_updates        | 29974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=21.16 +/- 5.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=17.28 +/- 5.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.91     |\n",
      "|    n_updates        | 30124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 805      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 170900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.84     |\n",
      "|    n_updates        | 30224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=19.61 +/- 7.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.507    |\n",
      "|    n_updates        | 30249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=171500, episode_reward=17.39 +/- 5.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.32     |\n",
      "|    n_updates        | 30374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total_timesteps  | 171900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.385    |\n",
      "|    n_updates        | 30474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=21.86 +/- 5.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=15.55 +/- 6.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 30624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 172900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.977    |\n",
      "|    n_updates        | 30724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=22.16 +/- 2.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.348    |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=14.25 +/- 7.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.258    |\n",
      "|    n_updates        | 30874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 802      |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 173900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.812    |\n",
      "|    n_updates        | 30974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=15.89 +/- 5.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 30999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=15.20 +/- 2.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 31124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 802      |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total_timesteps  | 174900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 31224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=18.98 +/- 2.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 31249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=18.08 +/- 8.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.772    |\n",
      "|    n_updates        | 31374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 801      |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 175900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 31474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=20.29 +/- 3.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 31499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=23.49 +/- 4.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 31624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 800      |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total_timesteps  | 176900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 31724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=23.57 +/- 2.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.366    |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=16.05 +/- 5.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.381    |\n",
      "|    n_updates        | 31874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 800      |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 177900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.205    |\n",
      "|    n_updates        | 31974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=12.48 +/- 5.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 31999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=-27.25 +/- 4.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -27.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.248    |\n",
      "|    n_updates        | 32124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 799      |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total_timesteps  | 178900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.365    |\n",
      "|    n_updates        | 32224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-145.28 +/- 27.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -145     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 32249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=3.52 +/- 9.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 3.52     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.659    |\n",
      "|    n_updates        | 32374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 798      |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 179900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.628    |\n",
      "|    n_updates        | 32474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=2.57 +/- 5.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=14.44 +/- 4.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0962   |\n",
      "|    n_updates        | 32624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 798      |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 180900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 32724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=21.02 +/- 2.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 32749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=17.11 +/- 4.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.533    |\n",
      "|    n_updates        | 32874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 797      |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 181900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.864    |\n",
      "|    n_updates        | 32974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=182000, episode_reward=16.50 +/- 3.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.75     |\n",
      "|    n_updates        | 32999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=14.27 +/- 9.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 33124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 229      |\n",
      "|    total_timesteps  | 182900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 33224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=20.82 +/- 8.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.561    |\n",
      "|    n_updates        | 33249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=-99.57 +/- 17.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -99.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 33374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 11       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 183900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.824    |\n",
      "|    n_updates        | 33474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=22.82 +/- 2.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 33499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=20.33 +/- 4.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 33624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 184900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.637    |\n",
      "|    n_updates        | 33724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=24.96 +/- 2.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=21.23 +/- 3.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.624    |\n",
      "|    n_updates        | 33874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total_timesteps  | 185900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 33974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=20.18 +/- 4.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.428    |\n",
      "|    n_updates        | 33999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=24.66 +/- 3.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 34124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total_timesteps  | 186900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.713    |\n",
      "|    n_updates        | 34224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=15.19 +/- 3.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.468    |\n",
      "|    n_updates        | 34249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=187500, episode_reward=21.33 +/- 7.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 34374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 236      |\n",
      "|    total_timesteps  | 187900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 34474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=10.44 +/- 6.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.496    |\n",
      "|    n_updates        | 34499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-18.45 +/- 15.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 34624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 188900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 34724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=21.53 +/- 7.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.246    |\n",
      "|    n_updates        | 34749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=21.20 +/- 3.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 34874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.96     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 239      |\n",
      "|    total_timesteps  | 189900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.544    |\n",
      "|    n_updates        | 34974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=17.99 +/- 2.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.93     |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=9.28 +/- 5.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.575    |\n",
      "|    n_updates        | 35124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 190900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 35224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=14.99 +/- 6.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.552    |\n",
      "|    n_updates        | 35249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=-47.95 +/- 18.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -48      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 35374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 9.88     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total_timesteps  | 191900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 35474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=15.08 +/- 5.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.97     |\n",
      "|    n_updates        | 35499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=22.15 +/- 5.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 35624    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 192900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.595    |\n",
      "|    n_updates        | 35724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=20.34 +/- 5.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.923    |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=13.81 +/- 6.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 35874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 193900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.168    |\n",
      "|    n_updates        | 35974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=8.30 +/- 3.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.3      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 35999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=21.91 +/- 7.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 36124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 246      |\n",
      "|    total_timesteps  | 194900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 36224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=20.88 +/- 2.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.02     |\n",
      "|    n_updates        | 36249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=9.26 +/- 5.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.26     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.824    |\n",
      "|    n_updates        | 36374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 248      |\n",
      "|    total_timesteps  | 195900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 36474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-38.61 +/- 24.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -38.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.64     |\n",
      "|    n_updates        | 36499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=22.88 +/- 3.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 36624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total_timesteps  | 196900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 36724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=25.19 +/- 3.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=21.61 +/- 5.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 36874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 251      |\n",
      "|    total_timesteps  | 197900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.573    |\n",
      "|    n_updates        | 36974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=198000, episode_reward=17.10 +/- 5.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.623    |\n",
      "|    n_updates        | 36999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=9.70 +/- 3.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.878    |\n",
      "|    n_updates        | 37124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 198900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 37224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=16.71 +/- 6.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 37249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=19.19 +/- 7.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.397    |\n",
      "|    n_updates        | 37374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 10.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 199900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.432    |\n",
      "|    n_updates        | 37474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=8.37 +/- 8.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1b9158bb648>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "\n",
    "print(\"Модель не найдена! Начало обучения...\")\n",
    "policy_kwargs = dict(net_arch=[10,10])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1, gamma=1.0, tensorboard_log=\"./logs/\")\n",
    "total_timesteps = 200000\n",
    "model.learn(total_timesteps=total_timesteps,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7cc66d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "model = DQN.load(\"./logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a628b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимальный агент политики по Авалленду\n",
    "\n",
    "def spread_func(beta, sigma, k):\n",
    "    return lambda T_t: spread(beta, sigma, T_t, k) \n",
    "\n",
    "def r_func(sigma, beta):\n",
    "    return lambda T_t, s, q: r(beta, sigma, T_t, s, q)\n",
    "    \n",
    "class OptimalAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        self.r_func = r_func(sigma, beta)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        spread = self.spread_func(observation[2])\n",
    "        r_ = self.r_func(observation[2], observation[0], observation[1])\n",
    "        \n",
    "        bid = r_ - spread/2\n",
    "        ask = r_ + spread/2\n",
    "\n",
    "        ds = observation[0] - r_\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#agent = OptimalAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43ad49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# симметричный агент политики согласно Авалленду\n",
    "\n",
    "class SymmetricAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        #spread = self.spread_func(observation[2])\n",
    "        return 0\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#symmetric_agent = SymmetricAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ce61c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env_agent_comp(envs, agent_rl,agent_opt,agent_sym):\n",
    "    \n",
    "    env = envs[0]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_rl = np.zeros(env.n)\n",
    "    asks_rl = np.zeros(env.n)\n",
    "    ss_rl = np.zeros(env.n)\n",
    "    ws_rl = np.zeros(env.n)\n",
    "    qs_rl = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_rl = 0.0\n",
    "    while not final:\n",
    "\n",
    "        action_rl = agent_rl.predict(obs,deterministic=True)\n",
    "        ss_rl[i] = obs[0]\n",
    "        qs_rl[i] = obs[1]\n",
    "        \n",
    "        despl = (action_rl[0]-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        ba_spread = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "\n",
    "        bids_rl[i] = ss_rl[i] - despl - ba_spread/2\n",
    "        asks_rl[i] = ss_rl[i] - despl + ba_spread/2\n",
    "\n",
    "        obs, reward, final, w_rl = env.step(action_rl[0])\n",
    "        i += 1\n",
    "        total_reward_rl += reward\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "    env = envs[1]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_opt = np.zeros(env.n)\n",
    "    asks_opt = np.zeros(env.n)\n",
    "    ds_opt = np.zeros(env.n)\n",
    "    spread_opt = np.zeros(env.n)\n",
    "    ss_opt = np.zeros(env.n)\n",
    "    ws_opt = np.zeros(env.n)\n",
    "    qs_opt = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_opt = 0.0\n",
    "    while not final:\n",
    "        action_opt = agent_opt.step(obs)\n",
    "\n",
    "        ds_opt[i] = action_opt\n",
    "        spread_opt[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_opt[i] = obs[0]\n",
    "        qs_opt[i] = obs[1]\n",
    "\n",
    "        bids_opt[i] = ss_opt[i] - ds_opt[i] - spread_opt[i]/2\n",
    "        asks_opt[i] = ss_opt[i] - ds_opt[i] + spread_opt[i]/2\n",
    "\n",
    "        obs, reward, final, w_opt = env.step(action_opt)\n",
    "        total_reward_opt += reward\n",
    "        i += 1\n",
    "\n",
    "    env = envs[2]\n",
    "\n",
    "    obs = env.reset()\n",
    "    bids_sym = np.zeros(env.n)\n",
    "    asks_sym = np.zeros(env.n)\n",
    "    ds_sym = np.zeros(env.n)\n",
    "    spread_sym = np.zeros(env.n)\n",
    "    ss_sym = np.zeros(env.n)\n",
    "    ws_sym = np.zeros(env.n)\n",
    "    qs_sym = np.zeros(env.n)    \n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_sym = 0.0\n",
    "    while not final:\n",
    "        action_sym = agent_sym.step(obs)\n",
    "\n",
    "        ds_sym[i] = action_sym\n",
    "        spread_sym[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_sym[i] = obs[0]\n",
    "        qs_sym[i] = obs[1]\n",
    "\n",
    "        bids_sym[i] = ss_sym[i] - ds_sym[i] - spread_sym[i]/2\n",
    "        asks_sym[i] = ss_sym[i] - ds_sym[i] + spread_sym[i]/2\n",
    "        \n",
    "        obs, reward, final, w_sym = env.step(action_sym)\n",
    "        i += 1\n",
    "        total_reward_sym += reward\n",
    "\n",
    "        \n",
    "    return w_rl['w'], w_opt['w'], w_sym['w'],total_reward_rl,total_reward_opt,total_reward_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecdc5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "1.0%\n",
      "2.0%\n",
      "3.0%\n",
      "4.0%\n",
      "5.0%\n",
      "6.0%\n",
      "7.0%\n",
      "8.0%\n",
      "9.0%\n",
      "10.0%\n",
      "11.0%\n",
      "12.0%\n",
      "13.0%\n",
      "14.0%\n",
      "15.0%\n",
      "16.0%\n",
      "17.0%\n",
      "18.0%\n",
      "19.0%\n",
      "20.0%\n",
      "21.0%\n",
      "22.0%\n",
      "23.0%\n",
      "24.0%\n",
      "25.0%\n",
      "26.0%\n",
      "27.0%\n",
      "28.0%\n",
      "29.0%\n",
      "30.0%\n",
      "31.0%\n",
      "32.0%\n",
      "33.0%\n",
      "34.0%\n",
      "35.0%\n",
      "36.0%\n",
      "37.0%\n",
      "38.0%\n",
      "39.0%\n",
      "40.0%\n",
      "41.0%\n",
      "42.0%\n",
      "43.0%\n",
      "44.0%\n",
      "45.0%\n",
      "46.0%\n",
      "47.0%\n",
      "48.0%\n",
      "49.0%\n",
      "50.0%\n",
      "51.0%\n",
      "52.0%\n",
      "53.0%\n",
      "54.0%\n",
      "55.0%\n",
      "56.0%\n",
      "57.0%\n",
      "58.0%\n",
      "59.0%\n",
      "60.0%\n",
      "61.0%\n",
      "62.0%\n",
      "63.0%\n",
      "64.0%\n",
      "65.0%\n",
      "66.0%\n",
      "67.0%\n",
      "68.0%\n",
      "69.0%\n",
      "70.0%\n",
      "71.0%\n",
      "72.0%\n",
      "73.0%\n",
      "74.0%\n",
      "75.0%\n",
      "76.0%\n",
      "77.0%\n",
      "78.0%\n",
      "79.0%\n",
      "80.0%\n",
      "81.0%\n",
      "82.0%\n",
      "83.0%\n",
      "84.0%\n",
      "85.0%\n",
      "86.0%\n",
      "87.0%\n",
      "88.0%\n",
      "89.0%\n",
      "90.0%\n",
      "91.0%\n",
      "92.0%\n",
      "93.0%\n",
      "94.0%\n",
      "95.0%\n",
      "96.0%\n",
      "97.0%\n",
      "98.0%\n",
      "99.0%\n"
     ]
    }
   ],
   "source": [
    "number_of_sims = 1000\n",
    "\n",
    "n = int(T/dt)\n",
    "ws_rl = np.zeros(number_of_sims)\n",
    "ws_opt = np.zeros(number_of_sims)\n",
    "ws_sym = np.zeros(number_of_sims)\n",
    "tr_rl = np.zeros(number_of_sims)\n",
    "tr_opt = np.zeros(number_of_sims)\n",
    "tr_sym = np.zeros(number_of_sims)\n",
    "\n",
    "envs = [Environment(s0, T, dt, sigma, beta, k, A, kappa),Environment(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False),Environment(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False)]\n",
    "for i in range(number_of_sims):\n",
    "    if i%10 == 0:\n",
    "        print(str(i/10) + \"%\")\n",
    "    ws_rl[i], ws_opt[i], ws_sym[i], tr_rl[i], tr_opt[i], tr_sym[i] = run_env_agent_comp(envs, model,OptimalAgent(beta, sigma, k),SymmetricAgent(beta, sigma, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a26a4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Гистограмма накопленного Зароботка')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure 2 (p. 222)\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([ws_opt,ws_rl], bins=30,edgecolor='black', label=['Оптимальное','Дискретное RL'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Зароботок\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного Зароботка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf70d308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Гистограмма накопленного зароботка')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_sym,tr_rl], bins=30,edgecolor='black', label=['Оптимальное', 'Симметричное','Дискретное обучение с подкреплением'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Общая награда\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного зароботка\")\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_rl], bins=30,edgecolor='black', label=['Оптимальное','Дискретное обучение с подкреплением'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Общая награда\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного зароботка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96a02785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное:\n",
      "47.79761695404737\n",
      "6.099978058681764\n",
      "7.83570309503321\n",
      "Симетричное:\n",
      "57.676984427753716\n",
      "11.86348257940186\n",
      "4.8617245435076715\n",
      "RL:\n",
      "49.90249457974856\n",
      "6.89669195967651\n",
      "7.235714581935488\n",
      "\n",
      "Оптимальное значение функции полезности: \t-2.6283680458275698e-09\n",
      "Значение симметричной функции полезности: \t-4.34167929750502e-06\n",
      "Значение функции полезности RL: \t\t-1.6997325578463324e-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Оптимальное:\")\n",
    "print(np.mean(ws_opt))\n",
    "print(np.std(ws_opt))\n",
    "print(np.mean(ws_opt)/np.std(ws_opt))\n",
    "print(\"Симетричное:\")\n",
    "print(np.mean(ws_sym))\n",
    "print(np.std(ws_sym))\n",
    "print(np.mean(ws_sym)/np.std(ws_sym))\n",
    "print(\"RL:\")\n",
    "print(np.mean(ws_rl))\n",
    "print(np.std(ws_rl))\n",
    "print(np.mean(ws_rl)/np.std(ws_rl))\n",
    "\n",
    "print()\n",
    "\n",
    "utility_avellaneda = np.mean(-np.exp(-beta*ws_opt))\n",
    "utility_rl = np.mean(-np.exp(-beta*ws_rl))\n",
    "\n",
    "print(\"Оптимальное значение функции полезности: \\t{}\".format(utility_avellaneda))\n",
    "print(\"Значение симметричной функции полезности: \\t{}\".format(np.mean(-np.exp(-beta*ws_sym))))\n",
    "print(\"Значение функции полезности RL: \\t\\t{}\".format(utility_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56ac6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное:\n",
      "22.46050972128678\n",
      "3.4646472377108304\n",
      "6.482769580930536\n",
      "Симетричное:\n",
      "-7.172303914746957\n",
      "42.1491264960118\n",
      "-0.1701649479123988\n",
      "RL:\n",
      "26.553152166909168\n",
      "4.383762309625215\n",
      "6.057160560144353\n"
     ]
    }
   ],
   "source": [
    "print(\"Оптимальное:\")\n",
    "print(np.mean(tr_opt))\n",
    "print(np.std(tr_opt))\n",
    "print(np.mean(tr_opt)/np.std(tr_opt))\n",
    "\n",
    "print(\"Симетричное:\")\n",
    "print(np.mean(tr_sym))\n",
    "print(np.std(tr_sym))\n",
    "print(np.mean(tr_sym)/np.std(tr_sym))\n",
    "\n",
    "print(\"RL:\")\n",
    "print(np.mean(tr_rl))\n",
    "print(np.std(tr_rl))\n",
    "print(np.mean(tr_rl)/np.std(tr_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5483a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_ws.csv', 'w') as f: \n",
    "      \n",
    "    # используя метод csv.writer из пакета CSV\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(ws_rl):\n",
    "        write.writerow([ws_rl[index], ws_opt[index],ws_sym[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab5219fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_tr.csv', 'w') as f: \n",
    "      \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(tr_rl):\n",
    "        write.writerow([tr_rl[index], tr_opt[index],tr_sym[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d7bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
