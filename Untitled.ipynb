{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26d58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "# сделаем симуляцию в среде RL:\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from runstats import *\n",
    "import runstats\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bc3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petro\\opencv\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "# использование дискретного пространства действий, аналогичного документу «Создание рынка с помощью обучения с подкреплением»   (https://arxiv.org/pdf/1804.04216.pdf)\n",
    "actions_num = 21   \n",
    "max_abs_dif = 4\n",
    "max_abs_spread = 20\n",
    "\n",
    "\n",
    "s0 = 100\n",
    "T = 1. # Общее время.\n",
    "sigma = 2.  # Среднеквадратичное отклонение.\n",
    "dt = .005  # Шаг времени.\n",
    "beta = 0.5\n",
    "kappa = beta * 2\n",
    "k = 1.5\n",
    "A = 137.45\n",
    "\n",
    "def spread(beta, sigma, T_t, k):\n",
    "    return beta*sigma**2*(T_t) + 2/beta*np.log(1+beta/k)\n",
    "\n",
    "def r(beta, sigma, T_t, s, q):\n",
    "    return s - q*beta*sigma**2*(T_t)\n",
    "\n",
    "def l(A, k, d):\n",
    "    '''\n",
    "    A : float\n",
    "        согласно Авалленду A = \\lambda/\\alpha, где alpha такая же, как указано выше, \n",
    "        lambda — постоянная частота рыночных ордеров на покупку и продажу.\n",
    "    k : float\n",
    "        согласно Авалленду k = alpha*K, где alpha ~ 1.5, и K таково, что \\delta p ~ Kln(Q) для рыночного ордера размера Q\n",
    "    d : float\n",
    "        согласно Авалленду, d=расстояние до средней цены\n",
    "    \n",
    "    l : float:\n",
    "        согласно Авалленду, l = lambda = интенсивность Пуассона, при которой выполняются приказы нашего агента.\n",
    "    '''\n",
    "    return A*np.exp(-k*d)   \n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, s0, T, dt, sigma, beta, k, A, kappa, seed=0, is_discrete=True):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : float\n",
    "            Начальное значение цены фьючерса/акции.\n",
    "        b : float\n",
    "            Начальное значение «бреча».\n",
    "        T : float\n",
    "            Общее время.\n",
    "        dt : float\n",
    "            Шаг времени.\n",
    "        sigma : float\n",
    "            Волатильность цен.\n",
    "        gamma : float\n",
    "            Фактор дисконта.\n",
    "        k : float\n",
    "            согласно Авалленду k = alpha*K, где alpha ~ 1.5, и K таково, что \\delta p ~ Kln(Q) для рыночного ордера размера Q\n",
    "        A : float\n",
    "            согласно Авалленду A = \\lambda/\\alpha, где alpha такая же, как указано выше, \n",
    "            lambda — постоянная частота рыночных ордеров на покупку и продажу.\n",
    "    \n",
    "        '''\n",
    "        self.s0 = s0\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        self.A = A\n",
    "        self.sqrtdt = np.sqrt(dt)\n",
    "        self.kappa = kappa\n",
    "        self.is_discrete = is_discrete\n",
    "        self.stats = runstats.ExponentialStatistics(decay=0.999)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # пространство наблюдения: s (цена), q, T-t (оставшееся время)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0.0, -math.inf, 0.0]),\n",
    "                                     high=np.array([math.inf, math.inf,T]),\n",
    "                                     dtype=np.float32)\n",
    "        # пространство действия: spread, ds\n",
    "        self.action_space = gym.spaces.Discrete(actions_num)\n",
    "        self.reward_range = (-math.inf,math.inf)\n",
    "        \n",
    "        self.metadata = None\n",
    "        \n",
    "    def reset(self,seed=0):\n",
    "        self.s = self.s0\n",
    "        self.q = 0.0\n",
    "        self.t = 0.0\n",
    "        self.w = 0.0\n",
    "        self.n = int(T/dt)\n",
    "        self.c_ = 0.0\n",
    "        return np.array((self.s,self.q,self.T))\n",
    "        \n",
    "    def step(self, action):\n",
    "        if self.is_discrete:\n",
    "            despl = (action-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        else:\n",
    "            despl = action\n",
    "        ba_spread = spread(self.beta,self.sigma,self.T-self.t,self.k)\n",
    "\n",
    "        bid = self.s - despl - ba_spread/2\n",
    "        ask = self.s - despl + ba_spread/2\n",
    "                \n",
    "        db = self.s - bid\n",
    "        da = ask - self.s\n",
    "        \n",
    "        lb = l(A, k, db)\n",
    "        la = l(A, k, da)\n",
    "        \n",
    "        dnb = 1 if np.random.uniform() <= lb * self.dt else 0\n",
    "        dna = 1 if np.random.uniform() <= la * self.dt else 0\n",
    "        self.q += dnb - dna\n",
    "\n",
    "        self.c_ += -dnb * bid + dna * ask # заработок\n",
    "\n",
    "        self.s += self.sigma * self.sqrtdt *(1 if np.random.uniform() < 0.5 else -1)\n",
    "\n",
    "        previous_w = self.w\n",
    "        self.w = self.c_ + self.q * self.s\n",
    "                \n",
    "        dw = (self.w - previous_w)\n",
    "        self.stats.push(dw)\n",
    "\n",
    "        reward = dw - self.kappa/2 * (dw - self.stats.mean())**2\n",
    "        \n",
    "        self.t += self.dt\n",
    "            \n",
    "        return np.array((self.s,self.q,self.T-self.t)), reward, self.t >= self.T, {'w':self.w}\n",
    "    \n",
    "env = Environment(s0, T, dt, sigma, beta, k, A,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73653fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузка расширения ноутбука TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beeb65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель не найдена! Начало обучения...\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/DQN_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petro\\opencv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-3812.93 +/- 594.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.81e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.976     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 500       |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | -174     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1819     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 900      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-3480.96 +/- 714.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.953     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 1000      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-3535.17 +/- 376.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.54e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.929     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 1500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1456     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-3869.92 +/- 507.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 2000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-3477.39 +/- 605.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.881     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 2500      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 242      |\n",
      "|    ep_rew_mean      | -215     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1394     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-3576.61 +/- 442.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.58e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.858     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 3000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-3356.52 +/- 739.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.36e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.834     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 3500      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 244      |\n",
      "|    ep_rew_mean      | -250     |\n",
      "|    exploration_rate | 0.815    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1357     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-3277.60 +/- 678.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.81      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 4000      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=-3906.30 +/- 1084.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.786     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 4500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 245      |\n",
      "|    ep_rew_mean      | -248     |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1350     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 4900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-3967.54 +/- 490.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.97e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.763     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-3757.44 +/- 759.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.76e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.739     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 5500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -242     |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1348     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 5900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-3869.14 +/- 877.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.715     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 6000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-3819.74 +/- 280.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.82e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.691     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 6500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 246      |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 6900     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=7000, episode_reward=-4110.87 +/- 537.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.11e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.668     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 7000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-3669.43 +/- 667.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.67e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.644     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 7500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -220     |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 7900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-3461.91 +/- 663.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.62      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 8000      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-3713.63 +/- 806.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.71e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.596     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 8500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 247      |\n",
      "|    ep_rew_mean      | -229     |\n",
      "|    exploration_rate | 0.577    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 8900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-3183.85 +/- 423.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.18e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.573     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 9000      |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9500, episode_reward=-3714.90 +/- 525.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.71e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.549     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 9500      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.53     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 9900     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-3925.41 +/- 631.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.93e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.525     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 10000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-3498.78 +/- 400.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.5e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.482    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 10900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-3446.57 +/- 557.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.45e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.478     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 11000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-3710.31 +/- 210.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.71e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.454     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 11500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -231     |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 11900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-3826.97 +/- 897.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.83e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.43      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 12000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-3076.54 +/- 550.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.08e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.406     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 12500     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -225     |\n",
      "|    exploration_rate | 0.387    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 12900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-4202.73 +/- 756.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.2e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-3989.51 +/- 856.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.99e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.359     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 13500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -227     |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1330     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 13900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=14000, episode_reward=-3773.76 +/- 754.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.77e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.335     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 14000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-3429.67 +/- 544.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.43e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.311     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 14500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -224     |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 14900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-3543.96 +/- 442.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.54e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.288     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-3249.79 +/- 715.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.25e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.264     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 15500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 248      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 15900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-3648.89 +/- 397.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.65e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 16000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-3066.22 +/- 435.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.216     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 16500     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -220     |\n",
      "|    exploration_rate | 0.197    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 16900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-3509.51 +/- 809.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.51e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.193     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 17000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-3967.65 +/- 853.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.97e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.169     |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 17500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -220     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 17900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-3402.36 +/- 260.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.4e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-4003.04 +/- 799.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4e+03   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.121    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -215     |\n",
      "|    exploration_rate | 0.102    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 18900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-3044.57 +/- 321.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.0975    |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 19000     |\n",
      "-----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19500, episode_reward=-4095.86 +/- 667.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.1e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0738   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.0547   |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1330     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 19900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-3597.24 +/- 475.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.6e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-3464.78 +/- 416.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 20500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -208     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 20900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=21000, episode_reward=-3418.83 +/- 940.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.42e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 21000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-3930.16 +/- 626.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.93e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 21500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -206     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1332     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 21900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-3939.12 +/- 428.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.94e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 22000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-3349.52 +/- 299.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.35e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 22500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1333     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 22900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-3863.99 +/- 937.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.86e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 23000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-3303.05 +/- 326.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1334     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 23900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-3834.04 +/- 503.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.83e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 24000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-3795.64 +/- 598.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.8e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 249      |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1334     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 24900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-4065.35 +/- 468.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-3836.75 +/- 645.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.84e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 25500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 1335     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 25900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-3991.38 +/- 602.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.99e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 26000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-3571.19 +/- 385.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.57e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 26500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 1336     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 26900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-3329.38 +/- 362.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 27000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-3745.27 +/- 660.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.75e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 27500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -198     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 1337     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 27900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=28000, episode_reward=-3925.15 +/- 660.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.93e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 28000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=-4380.39 +/- 1194.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.38e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 28500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -193     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 1337     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 28900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-3885.25 +/- 399.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.89e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 29000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=-4299.36 +/- 675.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1338     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 29900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-4092.49 +/- 426.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-4122.46 +/- 390.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.12e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 30500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -201     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1338     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 30900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-3139.95 +/- 379.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.14e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 31000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-4086.91 +/- 831.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 31500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 1339     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 31900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-4024.36 +/- 363.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.02e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 32000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-3670.51 +/- 616.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.67e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 32500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -201     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 1339     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 32900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-4282.62 +/- 279.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 33000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-4272.16 +/- 595.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.27e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 33500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -197     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 1339     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 33900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-4338.50 +/- 607.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.34e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 34000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-3942.76 +/- 638.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.94e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 34500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -202     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 34900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=-3804.58 +/- 772.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.8e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-3334.76 +/- 648.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 35500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -203     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 35900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-3506.62 +/- 686.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.51e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 36000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-3841.21 +/- 405.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.84e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 36500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -200     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 36900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-3914.28 +/- 182.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 37000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=-3775.29 +/- 579.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.78e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 37500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -205     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 37900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-3926.56 +/- 643.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.93e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 38000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=-3792.62 +/- 692.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.79e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 38500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 38900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-3588.83 +/- 390.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.59e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 39000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=-3919.99 +/- 479.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.92e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 39500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 39900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-3401.68 +/- 478.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.4e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=-4169.85 +/- 555.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.17e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 40500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -210     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 40900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-3501.50 +/- 383.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.5e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=-3300.63 +/- 620.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.3e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -211     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 41900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=42000, episode_reward=-3375.77 +/- 564.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.38e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 42000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-3827.88 +/- 332.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.83e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 42500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -214     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 42900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-3455.29 +/- 438.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 43000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=-3748.94 +/- 473.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.75e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 43500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -216     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 43900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-4151.29 +/- 557.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.15e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 44000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-4005.40 +/- 677.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.01e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 44500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -218     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 44900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-3519.34 +/- 533.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.52e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 45000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-3796.04 +/- 852.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.8e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45500    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 45900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-3600.49 +/- 517.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.6e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-3919.61 +/- 101.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.92e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 46500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -222     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 46900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-3579.42 +/- 401.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.58e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 47000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-3646.10 +/- 936.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.65e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 47500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -227     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 47900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-3873.26 +/- 709.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 48000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-3659.03 +/- 670.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.66e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 48500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -228     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 48900    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=49000, episode_reward=-4022.83 +/- 788.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -4.02e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 49000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-3565.17 +/- 595.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.57e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 49500     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -223     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 1342     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 49900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-3848.64 +/- 643.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.85e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50000     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-3813.83 +/- 286.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.81e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 50500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 4.95      |\n",
      "|    n_updates        | 124       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -374     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 1320     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 50900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.03     |\n",
      "|    n_updates        | 224      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-3632.24 +/- 738.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.63e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 51000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 5.29      |\n",
      "|    n_updates        | 249       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-3381.20 +/- 533.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -3.38e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    total_timesteps  | 51500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 5.41      |\n",
      "|    n_updates        | 374       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -456     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1298     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 51900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.31     |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-949.90 +/- 96.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -950     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.38     |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52500, episode_reward=-720.79 +/- 66.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -721     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.57     |\n",
      "|    n_updates        | 624      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -479     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1276     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 52900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.52     |\n",
      "|    n_updates        | 724      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=3.36 +/- 5.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 3.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.81     |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53500, episode_reward=-626.70 +/- 94.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -627     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.3      |\n",
      "|    n_updates        | 874      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -482     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1257     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 53900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.16     |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-77.18 +/- 3.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -77.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.14     |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=54500, episode_reward=-174.73 +/- 7.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -175     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3        |\n",
      "|    n_updates        | 1124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -469     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1239     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 54900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.75     |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-68.37 +/- 4.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -68.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.04     |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-75.79 +/- 9.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -75.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 1374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -466     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 1222     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 55900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 1474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-57.75 +/- 12.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -57.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=-92.49 +/- 4.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -92.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.988    |\n",
      "|    n_updates        | 1624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -465     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 1206     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 56900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.61     |\n",
      "|    n_updates        | 1724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-96.25 +/- 9.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -96.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.749    |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=-160.30 +/- 9.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -160     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.453    |\n",
      "|    n_updates        | 1874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -464     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 1191     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 57900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.699    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-53.73 +/- 19.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -53.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.567    |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=-18.50 +/- 54.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.19     |\n",
      "|    n_updates        | 2124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -458     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 1177     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 58900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.961    |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=29.89 +/- 3.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 29.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.638    |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59500, episode_reward=20.75 +/- 6.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.512    |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -445     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 1163     |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 59900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=23.48 +/- 4.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.91     |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=-67.76 +/- 16.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.647    |\n",
      "|    n_updates        | 2624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -438     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 1151     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 60900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.827    |\n",
      "|    n_updates        | 2724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=27.19 +/- 2.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-30.75 +/- 74.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -30.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.659    |\n",
      "|    n_updates        | 2874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -427     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 1139     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 61900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 2974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=20.74 +/- 17.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 2999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=15.95 +/- 13.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.919    |\n",
      "|    n_updates        | 3124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -415     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 1128     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 62900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 3224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-10.69 +/- 8.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.637    |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=20.50 +/- 24.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.704    |\n",
      "|    n_updates        | 3374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -402     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 1117     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 63900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.05     |\n",
      "|    n_updates        | 3474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=27.88 +/- 2.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.41     |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=-1.64 +/- 32.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.64    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 3624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -396     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 1107     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 64900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=65000, episode_reward=-19.20 +/- 35.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.62     |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=30.26 +/- 5.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 3874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -389     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 1097     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 65900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.851    |\n",
      "|    n_updates        | 3974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=28.95 +/- 2.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 29       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.598    |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=20.26 +/- 14.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 4124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -380     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 1088     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 66900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 4224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=27.22 +/- 6.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.371    |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=-33.55 +/- 14.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -33.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 4374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -369     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 1079     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 67900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.472    |\n",
      "|    n_updates        | 4474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=27.53 +/- 3.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.544    |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=25.02 +/- 5.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.708    |\n",
      "|    n_updates        | 4624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -362     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 1070     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 68900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.715    |\n",
      "|    n_updates        | 4724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=21.56 +/- 6.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.342    |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=27.90 +/- 3.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.671    |\n",
      "|    n_updates        | 4874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -355     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 1062     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 69900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.342    |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=30.70 +/- 3.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.24     |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=70500, episode_reward=21.09 +/- 3.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.832    |\n",
      "|    n_updates        | 5124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -344     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 1054     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 70900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.27     |\n",
      "|    n_updates        | 5224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=31.24 +/- 5.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 31.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.711    |\n",
      "|    n_updates        | 5249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71500, episode_reward=30.51 +/- 3.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 5374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -336     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 1047     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 71900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.788    |\n",
      "|    n_updates        | 5474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=31.04 +/- 5.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 31       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.791    |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=22.11 +/- 3.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.787    |\n",
      "|    n_updates        | 5624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -326     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 1040     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 72900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.727    |\n",
      "|    n_updates        | 5724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=27.16 +/- 3.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=28.64 +/- 6.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.942    |\n",
      "|    n_updates        | 5874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -317     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 1033     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 73900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 5974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=26.21 +/- 3.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.08     |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-59.94 +/- 32.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -59.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 6124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -311     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 1027     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 74900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.862    |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=11.33 +/- 6.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.814    |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=24.83 +/- 7.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.366    |\n",
      "|    n_updates        | 6374     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -151     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 1021     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 75900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.626    |\n",
      "|    n_updates        | 6474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=18.24 +/- 3.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.652    |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-4.75 +/- 20.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.75    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.682    |\n",
      "|    n_updates        | 6624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -63.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 1015     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 76900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 6724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-1.23 +/- 23.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.23    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.768    |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=26.78 +/- 3.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.66     |\n",
      "|    n_updates        | 6874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -33.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 1009     |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 77900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.361    |\n",
      "|    n_updates        | 6974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=28.40 +/- 4.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 6999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=28.39 +/- 4.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 28.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.367    |\n",
      "|    n_updates        | 7124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -21.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 1004     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 78900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.744    |\n",
      "|    n_updates        | 7224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=26.63 +/- 3.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.896    |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=11.19 +/- 7.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.3      |\n",
      "|    n_updates        | 7374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -14.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 998      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 79900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.758    |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=30.20 +/- 7.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 30.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.536    |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=-132.29 +/- 11.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -132     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.823    |\n",
      "|    n_updates        | 7624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -11.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 993      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 80900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.299    |\n",
      "|    n_updates        | 7724     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=81000, episode_reward=29.47 +/- 5.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 29.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.429    |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=29.85 +/- 4.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 29.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.904    |\n",
      "|    n_updates        | 7874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -8.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 988      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 81900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.927    |\n",
      "|    n_updates        | 7974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=17.58 +/- 8.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.602    |\n",
      "|    n_updates        | 7999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=24.30 +/- 6.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.652    |\n",
      "|    n_updates        | 8124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 983      |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 82900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 8224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=19.76 +/- 5.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 8249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=22.21 +/- 6.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 8374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 1.33     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 978      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 83900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.474    |\n",
      "|    n_updates        | 8474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-7.12 +/- 9.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.12    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.558    |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=21.29 +/- 4.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.798    |\n",
      "|    n_updates        | 8624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 2.81     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 973      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 84900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.523    |\n",
      "|    n_updates        | 8724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-64.37 +/- 42.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -64.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.763    |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=-107.59 +/- 30.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -108     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.861    |\n",
      "|    n_updates        | 8874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 4.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 969      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 85900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.48     |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=21.03 +/- 7.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.526    |\n",
      "|    n_updates        | 8999     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=86500, episode_reward=18.10 +/- 14.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.83     |\n",
      "|    n_updates        | 9124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 3.66     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 964      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 86900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 9224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=26.78 +/- 2.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.648    |\n",
      "|    n_updates        | 9249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=-80.32 +/- 59.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -80.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.761    |\n",
      "|    n_updates        | 9374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 4.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 960      |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 87900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 9474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=26.45 +/- 7.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-1.62 +/- 6.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.62    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.499    |\n",
      "|    n_updates        | 9624     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 3.82     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 956      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 88900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.357    |\n",
      "|    n_updates        | 9724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=13.61 +/- 17.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.518    |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=-20.00 +/- 34.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.663    |\n",
      "|    n_updates        | 9874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 6.83     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 952      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 89900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.486    |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=19.95 +/- 10.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=5.76 +/- 21.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 5.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.356    |\n",
      "|    n_updates        | 10124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 5.63     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 90900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.974    |\n",
      "|    n_updates        | 10224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=19.69 +/- 3.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.67     |\n",
      "|    n_updates        | 10249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=18.59 +/- 5.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.332    |\n",
      "|    n_updates        | 10374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 5.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 945      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 91900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.789    |\n",
      "|    n_updates        | 10474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=15.00 +/- 9.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.444    |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=13.83 +/- 5.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 10624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 5.46     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 942      |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 92900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.43     |\n",
      "|    n_updates        | 10724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-76.10 +/- 23.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -76.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.37     |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=24.95 +/- 5.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.347    |\n",
      "|    n_updates        | 10874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 5.45     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 938      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 93900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.364    |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=21.35 +/- 5.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.476    |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=11.88 +/- 7.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.49     |\n",
      "|    n_updates        | 11124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 5.45     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 934      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 94900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.541    |\n",
      "|    n_updates        | 11224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=11.25 +/- 15.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.393    |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=17.11 +/- 4.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.518    |\n",
      "|    n_updates        | 11374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 4.98     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 931      |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 95900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 11474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=18.70 +/- 3.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.411    |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=16.26 +/- 10.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 11624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 3.68     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 928      |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 96900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.555    |\n",
      "|    n_updates        | 11724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=97000, episode_reward=17.30 +/- 6.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=20.07 +/- 5.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.285    |\n",
      "|    n_updates        | 11874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 3.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 925      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 97900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.494    |\n",
      "|    n_updates        | 11974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=8.45 +/- 12.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.45     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.634    |\n",
      "|    n_updates        | 11999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=25.03 +/- 6.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.461    |\n",
      "|    n_updates        | 12124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 2.85     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 922      |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 98900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.651    |\n",
      "|    n_updates        | 12224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=26.23 +/- 3.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.704    |\n",
      "|    n_updates        | 12249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=15.13 +/- 6.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99500    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.362    |\n",
      "|    n_updates        | 12374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 2.11     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 919      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 99900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.34     |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=6.68 +/- 10.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.295    |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=22.90 +/- 4.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.922    |\n",
      "|    n_updates        | 12624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | 0.988    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 100900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 12724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=1.72 +/- 15.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 1.72     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.04     |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=18.51 +/- 14.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.604    |\n",
      "|    n_updates        | 12874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -0.393   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 913      |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 101900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 12974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-51.27 +/- 18.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -51.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.508    |\n",
      "|    n_updates        | 12999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=102500, episode_reward=7.09 +/- 13.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.564    |\n",
      "|    n_updates        | 13124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -1.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 911      |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 102900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.437    |\n",
      "|    n_updates        | 13224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-35.20 +/- 19.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -35.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 13249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=0.98 +/- 10.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.981    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.489    |\n",
      "|    n_updates        | 13374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -1.51    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 908      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 103900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.776    |\n",
      "|    n_updates        | 13474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-73.32 +/- 39.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -73.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-67.77 +/- 6.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 13624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 905      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 104900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.844    |\n",
      "|    n_updates        | 13724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-1.02 +/- 8.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.02    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.765    |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-31.71 +/- 29.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -31.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.271    |\n",
      "|    n_updates        | 13874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -2.45    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 903      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 105900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.316    |\n",
      "|    n_updates        | 13974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-215.87 +/- 123.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -216     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.837    |\n",
      "|    n_updates        | 13999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=-0.82 +/- 32.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 14124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -8.81    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 900      |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 106900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.892    |\n",
      "|    n_updates        | 14224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=2.94 +/- 11.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.94     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.83     |\n",
      "|    n_updates        | 14249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-7.81 +/- 10.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.642    |\n",
      "|    n_updates        | 14374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -10.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 898      |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 107900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.493    |\n",
      "|    n_updates        | 14474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-55.22 +/- 15.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -55.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.423    |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-89.98 +/- 27.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -90      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.571    |\n",
      "|    n_updates        | 14624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -13.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 896      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 108900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 14724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=15.05 +/- 7.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.845    |\n",
      "|    n_updates        | 14749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-32.90 +/- 32.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -32.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.316    |\n",
      "|    n_updates        | 14874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -15.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 109900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.515    |\n",
      "|    n_updates        | 14974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-52.34 +/- 12.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -52.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-22.82 +/- 44.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 15124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -18.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 891      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 110900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.874    |\n",
      "|    n_updates        | 15224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-64.96 +/- 48.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -65      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.691    |\n",
      "|    n_updates        | 15249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=19.00 +/- 1.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 15374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -20.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 889      |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 111900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.609    |\n",
      "|    n_updates        | 15474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-155.61 +/- 33.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -156     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-49.20 +/- 37.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -49.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.704    |\n",
      "|    n_updates        | 15624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -24.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 887      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 112900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.402    |\n",
      "|    n_updates        | 15724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=113000, episode_reward=-140.97 +/- 39.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -141     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.758    |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=-68.08 +/- 21.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -68.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.93     |\n",
      "|    n_updates        | 15874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -26.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 885      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 113900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.495    |\n",
      "|    n_updates        | 15974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-70.07 +/- 17.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -70.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 15999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=-157.26 +/- 17.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -157     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.493    |\n",
      "|    n_updates        | 16124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -29.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 883      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 114900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.298    |\n",
      "|    n_updates        | 16224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-2.27 +/- 27.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.27    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.927    |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=-29.11 +/- 22.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.868    |\n",
      "|    n_updates        | 16374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -28.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 881      |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 115900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.581    |\n",
      "|    n_updates        | 16474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-54.96 +/- 20.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -55      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=0.89 +/- 8.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.891    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.397    |\n",
      "|    n_updates        | 16624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -30.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 879      |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 116900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 16724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-56.94 +/- 40.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -56.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.371    |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=-73.19 +/- 37.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -73.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.205    |\n",
      "|    n_updates        | 16874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -31.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 877      |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 117900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 16974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-112.27 +/- 30.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.603    |\n",
      "|    n_updates        | 16999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=118500, episode_reward=-118.25 +/- 28.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -118     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.551    |\n",
      "|    n_updates        | 17124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -31.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 875      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 118900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.26     |\n",
      "|    n_updates        | 17224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-11.19 +/- 32.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=-12.49 +/- 15.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -12.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.407    |\n",
      "|    n_updates        | 17374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -33.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 874      |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 119900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.718    |\n",
      "|    n_updates        | 17474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-59.11 +/- 21.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -59.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.299    |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=-7.95 +/- 19.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.95    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.393    |\n",
      "|    n_updates        | 17624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -37.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 872      |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 120900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.684    |\n",
      "|    n_updates        | 17724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-16.90 +/- 12.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -16.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.471    |\n",
      "|    n_updates        | 17749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=-38.47 +/- 14.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -38.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.533    |\n",
      "|    n_updates        | 17874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -38.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 870      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 121900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.513    |\n",
      "|    n_updates        | 17974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-45.94 +/- 27.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -45.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 17999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=-6.29 +/- 19.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.29    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.378    |\n",
      "|    n_updates        | 18124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -40.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 869      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 122900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 18224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-10.38 +/- 16.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.584    |\n",
      "|    n_updates        | 18249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=-24.83 +/- 18.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -24.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 18374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -41.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 867      |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 123900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.463    |\n",
      "|    n_updates        | 18474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-33.15 +/- 20.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -33.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=-37.55 +/- 25.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -37.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.729    |\n",
      "|    n_updates        | 18624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -43.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 865      |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 124900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.78     |\n",
      "|    n_updates        | 18724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-47.43 +/- 34.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -47.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=-38.16 +/- 8.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -38.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.831    |\n",
      "|    n_updates        | 18874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -45.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 864      |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 125900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.69     |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-67.16 +/- 7.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 18999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=-62.99 +/- 29.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -63      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.336    |\n",
      "|    n_updates        | 19124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -46.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 862      |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 126900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 19224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-42.55 +/- 8.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.284    |\n",
      "|    n_updates        | 19249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=-23.63 +/- 23.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -23.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.365    |\n",
      "|    n_updates        | 19374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -47.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 861      |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 127900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.444    |\n",
      "|    n_updates        | 19474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-81.49 +/- 33.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -81.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.339    |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=-34.00 +/- 33.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -34      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.508    |\n",
      "|    n_updates        | 19624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 859      |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 128900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 19724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=129000, episode_reward=-15.09 +/- 22.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -15.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 19749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=6.08 +/- 16.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.369    |\n",
      "|    n_updates        | 19874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -51.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 857      |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 129900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.269    |\n",
      "|    n_updates        | 19974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-61.18 +/- 37.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -61.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=5.22 +/- 12.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 5.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 20124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -52      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 856      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 130900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.224    |\n",
      "|    n_updates        | 20224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-98.36 +/- 43.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -98.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.312    |\n",
      "|    n_updates        | 20249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=-24.80 +/- 41.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -24.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.647    |\n",
      "|    n_updates        | 20374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -47.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 854      |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 131900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.728    |\n",
      "|    n_updates        | 20474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-31.11 +/- 18.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -31.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.315    |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=-69.86 +/- 13.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -69.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.656    |\n",
      "|    n_updates        | 20624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 853      |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 132900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.348    |\n",
      "|    n_updates        | 20724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-52.27 +/- 39.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -52.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 20749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=-0.91 +/- 10.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.911   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 20874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 851      |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 133900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.623    |\n",
      "|    n_updates        | 20974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-56.70 +/- 22.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -56.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 20999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=134500, episode_reward=-10.53 +/- 22.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.793    |\n",
      "|    n_updates        | 21124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 850      |\n",
      "|    time_elapsed     | 158      |\n",
      "|    total_timesteps  | 134900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.803    |\n",
      "|    n_updates        | 21224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-120.24 +/- 27.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -120     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.733    |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=-73.53 +/- 11.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -73.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.612    |\n",
      "|    n_updates        | 21374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 849      |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 135900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.774    |\n",
      "|    n_updates        | 21474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-36.33 +/- 15.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -36.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.999    |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=-61.98 +/- 11.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -62      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 21624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 847      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 136900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 21724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-7.18 +/- 30.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.18    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.518    |\n",
      "|    n_updates        | 21749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=-57.35 +/- 33.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -57.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.301    |\n",
      "|    n_updates        | 21874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -49.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 846      |\n",
      "|    time_elapsed     | 162      |\n",
      "|    total_timesteps  | 137900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.929    |\n",
      "|    n_updates        | 21974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-42.20 +/- 20.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.32     |\n",
      "|    n_updates        | 21999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=-46.55 +/- 45.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -46.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.752    |\n",
      "|    n_updates        | 22124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -51.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 845      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 138900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.559    |\n",
      "|    n_updates        | 22224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-46.45 +/- 25.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -46.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.47     |\n",
      "|    n_updates        | 22249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=-41.43 +/- 52.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -41.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.552    |\n",
      "|    n_updates        | 22374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -50.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 844      |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 139900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.616    |\n",
      "|    n_updates        | 22474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-25.48 +/- 37.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -25.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.903    |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=-84.49 +/- 22.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -84.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.236    |\n",
      "|    n_updates        | 22624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -51.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 842      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 140900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.391    |\n",
      "|    n_updates        | 22724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-56.06 +/- 19.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -56.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.333    |\n",
      "|    n_updates        | 22749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=-93.81 +/- 20.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -93.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.374    |\n",
      "|    n_updates        | 22874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -53.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 841      |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 141900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.852    |\n",
      "|    n_updates        | 22974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-58.06 +/- 17.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -58.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.716    |\n",
      "|    n_updates        | 22999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=-121.33 +/- 9.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -121     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.435    |\n",
      "|    n_updates        | 23124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -55.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 840      |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 142900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.298    |\n",
      "|    n_updates        | 23224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-61.94 +/- 45.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -61.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.439    |\n",
      "|    n_updates        | 23249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=-4.25 +/- 30.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -4.25    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.459    |\n",
      "|    n_updates        | 23374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -55.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 839      |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 143900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.426    |\n",
      "|    n_updates        | 23474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-83.54 +/- 29.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -83.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.395    |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=-52.67 +/- 26.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -52.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 23624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -55.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 838      |\n",
      "|    time_elapsed     | 172      |\n",
      "|    total_timesteps  | 144900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 23724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=145000, episode_reward=-91.58 +/- 33.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -91.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.397    |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=-35.65 +/- 22.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -35.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.523    |\n",
      "|    n_updates        | 23874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -55.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 836      |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total_timesteps  | 145900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.487    |\n",
      "|    n_updates        | 23974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-40.33 +/- 38.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -40.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.614    |\n",
      "|    n_updates        | 23999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=-66.78 +/- 17.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -66.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.396    |\n",
      "|    n_updates        | 24124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -56.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 835      |\n",
      "|    time_elapsed     | 175      |\n",
      "|    total_timesteps  | 146900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 24224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-12.65 +/- 28.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -12.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.599    |\n",
      "|    n_updates        | 24249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=-37.20 +/- 13.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -37.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 24374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -56.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 833      |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total_timesteps  | 147900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.318    |\n",
      "|    n_updates        | 24474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-67.55 +/- 18.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.442    |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=-84.09 +/- 27.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -84.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 24624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -57      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 831      |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total_timesteps  | 148900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.992    |\n",
      "|    n_updates        | 24724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-72.86 +/- 25.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -72.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.664    |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=-91.88 +/- 16.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -91.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.61     |\n",
      "|    n_updates        | 24874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -57.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 829      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 149900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 24974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-35.85 +/- 31.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -35.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.387    |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=150500, episode_reward=-22.57 +/- 25.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.522    |\n",
      "|    n_updates        | 25124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -57.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 828      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 150900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.611    |\n",
      "|    n_updates        | 25224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-29.84 +/- 30.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 25249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=-51.12 +/- 19.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -51.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.809    |\n",
      "|    n_updates        | 25374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -57.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 827      |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 151900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.375    |\n",
      "|    n_updates        | 25474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-94.84 +/- 26.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -94.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.92     |\n",
      "|    n_updates        | 25499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=-95.59 +/- 28.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -95.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.448    |\n",
      "|    n_updates        | 25624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -57.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 826      |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total_timesteps  | 152900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.395    |\n",
      "|    n_updates        | 25724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-80.86 +/- 8.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -80.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.27     |\n",
      "|    n_updates        | 25749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=-95.97 +/- 27.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -96      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.311    |\n",
      "|    n_updates        | 25874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -59.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 824      |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 153900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.77     |\n",
      "|    n_updates        | 25974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-89.83 +/- 19.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -89.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.258    |\n",
      "|    n_updates        | 25999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=-160.06 +/- 27.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -160     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.43     |\n",
      "|    n_updates        | 26124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -61.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 823      |\n",
      "|    time_elapsed     | 188      |\n",
      "|    total_timesteps  | 154900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 26224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-9.12 +/- 25.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -9.12    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 26249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=-58.32 +/- 25.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -58.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.376    |\n",
      "|    n_updates        | 26374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -61.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 822      |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total_timesteps  | 155900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 26474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-38.60 +/- 34.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -38.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.297    |\n",
      "|    n_updates        | 26499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=-43.40 +/- 54.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -43.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.78     |\n",
      "|    n_updates        | 26624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -62.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 822      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 156900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.811    |\n",
      "|    n_updates        | 26724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-130.11 +/- 15.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -130     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.461    |\n",
      "|    n_updates        | 26749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=-65.96 +/- 20.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -66      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.723    |\n",
      "|    n_updates        | 26874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -61.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 821      |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total_timesteps  | 157900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.353    |\n",
      "|    n_updates        | 26974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-92.52 +/- 56.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -92.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.452    |\n",
      "|    n_updates        | 26999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=-68.81 +/- 43.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -68.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.466    |\n",
      "|    n_updates        | 27124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -62.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 820      |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 158900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 27224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-69.22 +/- 11.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -69.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.384    |\n",
      "|    n_updates        | 27249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=-85.26 +/- 9.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -85.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.468    |\n",
      "|    n_updates        | 27374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 819      |\n",
      "|    time_elapsed     | 195      |\n",
      "|    total_timesteps  | 159900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.545    |\n",
      "|    n_updates        | 27474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-39.07 +/- 54.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -39.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.582    |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=0.78 +/- 10.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.779    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.368    |\n",
      "|    n_updates        | 27624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -64.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 818      |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 160900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.244    |\n",
      "|    n_updates        | 27724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=161000, episode_reward=-13.51 +/- 14.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -13.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.733    |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=-50.50 +/- 13.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -50.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.289    |\n",
      "|    n_updates        | 27874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 817      |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total_timesteps  | 161900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.714    |\n",
      "|    n_updates        | 27974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-90.14 +/- 26.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -90.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.4      |\n",
      "|    n_updates        | 27999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=-119.80 +/- 36.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -120     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 28124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -66.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 162900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.8      |\n",
      "|    n_updates        | 28224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-8.09 +/- 15.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -8.09    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.372    |\n",
      "|    n_updates        | 28249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=-19.74 +/- 29.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -19.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.562    |\n",
      "|    n_updates        | 28374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -62.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 200      |\n",
      "|    total_timesteps  | 163900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 28474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-70.95 +/- 25.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -70.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 28499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=-18.90 +/- 16.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.69     |\n",
      "|    n_updates        | 28624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -64.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 815      |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total_timesteps  | 164900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.531    |\n",
      "|    n_updates        | 28724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-48.45 +/- 15.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -48.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=-92.52 +/- 19.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -92.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.535    |\n",
      "|    n_updates        | 28874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 814      |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 165900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.514    |\n",
      "|    n_updates        | 28974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-53.86 +/- 17.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -53.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.93     |\n",
      "|    n_updates        | 28999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=166500, episode_reward=-62.62 +/- 19.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -62.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.346    |\n",
      "|    n_updates        | 29124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -63.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 813      |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 166900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 29224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-35.61 +/- 29.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -35.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 29249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=-87.02 +/- 19.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -87      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.43     |\n",
      "|    n_updates        | 29374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 167900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.406    |\n",
      "|    n_updates        | 29474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-21.90 +/- 42.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -21.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 29499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=-41.95 +/- 20.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.547    |\n",
      "|    n_updates        | 29624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 207      |\n",
      "|    total_timesteps  | 168900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.505    |\n",
      "|    n_updates        | 29724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-90.43 +/- 36.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -90.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.57     |\n",
      "|    n_updates        | 29749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=-80.41 +/- 21.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -80.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.339    |\n",
      "|    n_updates        | 29874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -65.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 811      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 169900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.316    |\n",
      "|    n_updates        | 29974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-49.55 +/- 28.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -49.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=-41.04 +/- 26.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -41      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.62     |\n",
      "|    n_updates        | 30124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -64.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 810      |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 170900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.26     |\n",
      "|    n_updates        | 30224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-9.99 +/- 52.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -9.99    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 30249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=-71.41 +/- 36.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.834    |\n",
      "|    n_updates        | 30374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -64.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 171900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.422    |\n",
      "|    n_updates        | 30474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-22.11 +/- 28.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.351    |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=-139.71 +/- 33.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -140     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.295    |\n",
      "|    n_updates        | 30624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -67.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total_timesteps  | 172900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 30724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-39.83 +/- 16.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -39.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.396    |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=-92.34 +/- 37.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -92.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.447    |\n",
      "|    n_updates        | 30874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -67.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 808      |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 173900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.397    |\n",
      "|    n_updates        | 30974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-118.93 +/- 16.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.382    |\n",
      "|    n_updates        | 30999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=-50.35 +/- 9.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -50.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 31124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -68      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 174900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.5      |\n",
      "|    n_updates        | 31224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-75.71 +/- 26.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -75.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.529    |\n",
      "|    n_updates        | 31249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=-129.32 +/- 34.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -129     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.485    |\n",
      "|    n_updates        | 31374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -69.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 217      |\n",
      "|    total_timesteps  | 175900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 31474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-58.48 +/- 9.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -58.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.391    |\n",
      "|    n_updates        | 31499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=-69.80 +/- 40.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -69.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.548    |\n",
      "|    n_updates        | 31624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -69.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 806      |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 176900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.631    |\n",
      "|    n_updates        | 31724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=177000, episode_reward=-95.94 +/- 20.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -95.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.387    |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=-61.62 +/- 11.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -61.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.67     |\n",
      "|    n_updates        | 31874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -72.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 805      |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total_timesteps  | 177900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.35     |\n",
      "|    n_updates        | 31974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-61.78 +/- 16.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -61.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.471    |\n",
      "|    n_updates        | 31999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=-123.94 +/- 14.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -124     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.577    |\n",
      "|    n_updates        | 32124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -73.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 178900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 32224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-135.96 +/- 22.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -136     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.471    |\n",
      "|    n_updates        | 32249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=-70.64 +/- 10.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -70.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.87     |\n",
      "|    n_updates        | 32374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -72.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total_timesteps  | 179900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.435    |\n",
      "|    n_updates        | 32474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-66.59 +/- 15.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -66.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.898    |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=-116.15 +/- 35.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -116     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.411    |\n",
      "|    n_updates        | 32624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -74.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 180900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.315    |\n",
      "|    n_updates        | 32724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-71.01 +/- 10.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.869    |\n",
      "|    n_updates        | 32749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=-64.52 +/- 15.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -64.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.834    |\n",
      "|    n_updates        | 32874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -76.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 802      |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 181900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 32974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-8.74 +/- 29.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -8.74    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.35     |\n",
      "|    n_updates        | 32999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=182500, episode_reward=-109.22 +/- 34.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.532    |\n",
      "|    n_updates        | 33124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -81      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 801      |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 182900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 33224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-232.92 +/- 28.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -233     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 33249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=-79.83 +/- 11.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -79.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 33374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -80.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 800      |\n",
      "|    time_elapsed     | 229      |\n",
      "|    total_timesteps  | 183900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.448    |\n",
      "|    n_updates        | 33474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-50.89 +/- 28.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -50.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.4      |\n",
      "|    n_updates        | 33499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=-79.78 +/- 37.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -79.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.298    |\n",
      "|    n_updates        | 33624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -78.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 799      |\n",
      "|    time_elapsed     | 231      |\n",
      "|    total_timesteps  | 184900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.492    |\n",
      "|    n_updates        | 33724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-58.71 +/- 21.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -58.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.402    |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=-67.16 +/- 26.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -67.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.277    |\n",
      "|    n_updates        | 33874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -79.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 798      |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 185900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.39     |\n",
      "|    n_updates        | 33974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-83.08 +/- 39.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -83.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.65     |\n",
      "|    n_updates        | 33999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=-58.73 +/- 41.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -58.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.413    |\n",
      "|    n_updates        | 34124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -80.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 797      |\n",
      "|    time_elapsed     | 234      |\n",
      "|    total_timesteps  | 186900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.771    |\n",
      "|    n_updates        | 34224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-44.44 +/- 23.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -44.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 34249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=-63.81 +/- 45.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -63.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.384    |\n",
      "|    n_updates        | 34374    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -79.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total_timesteps  | 187900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.463    |\n",
      "|    n_updates        | 34474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-19.99 +/- 16.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -20      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.287    |\n",
      "|    n_updates        | 34499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-29.64 +/- 7.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.419    |\n",
      "|    n_updates        | 34624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -80.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 188900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.488    |\n",
      "|    n_updates        | 34724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-64.06 +/- 14.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -64.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.262    |\n",
      "|    n_updates        | 34749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=-62.94 +/- 55.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -62.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 34874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -79.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 189900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 34974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-29.01 +/- 14.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.523    |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=-144.79 +/- 98.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -145     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.741    |\n",
      "|    n_updates        | 35124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -82.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 190900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.269    |\n",
      "|    n_updates        | 35224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-36.43 +/- 22.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -36.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.335    |\n",
      "|    n_updates        | 35249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=-66.68 +/- 55.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -66.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.957    |\n",
      "|    n_updates        | 35374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -85.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 793      |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 191900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.668    |\n",
      "|    n_updates        | 35474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-77.08 +/- 38.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -77.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.75     |\n",
      "|    n_updates        | 35499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=-175.94 +/- 17.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -176     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 35624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -85.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 192900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.426    |\n",
      "|    n_updates        | 35724    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=193000, episode_reward=-70.96 +/- 9.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.867    |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=-40.14 +/- 21.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -40.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.545    |\n",
      "|    n_updates        | 35874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -87.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 193900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 35974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-75.07 +/- 57.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -75.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.416    |\n",
      "|    n_updates        | 35999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=-11.40 +/- 19.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 36124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -86.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 246      |\n",
      "|    total_timesteps  | 194900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.57     |\n",
      "|    n_updates        | 36224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-135.85 +/- 15.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -136     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.83     |\n",
      "|    n_updates        | 36249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=-153.83 +/- 16.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -154     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 36374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -89.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total_timesteps  | 195900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.956    |\n",
      "|    n_updates        | 36474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-94.08 +/- 57.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -94.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.596    |\n",
      "|    n_updates        | 36499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=-40.46 +/- 21.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -40.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.603    |\n",
      "|    n_updates        | 36624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -88.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total_timesteps  | 196900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.312    |\n",
      "|    n_updates        | 36724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-111.51 +/- 63.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=-69.27 +/- 31.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -69.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.341    |\n",
      "|    n_updates        | 36874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -87.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total_timesteps  | 197900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 36974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-55.39 +/- 30.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -55.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.301    |\n",
      "|    n_updates        | 36999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=198500, episode_reward=-102.68 +/- 69.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.74     |\n",
      "|    n_updates        | 37124    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -89.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 198900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.371    |\n",
      "|    n_updates        | 37224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-72.23 +/- 22.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -72.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 37249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=-59.34 +/- 48.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -59.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.677    |\n",
      "|    n_updates        | 37374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 250      |\n",
      "|    ep_rew_mean      | -89.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 199900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.576    |\n",
      "|    n_updates        | 37474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-31.16 +/- 14.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -31.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.325    |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x275e5c7cec8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "\n",
    "print(\"Модель не найдена! Начало обучения...\")\n",
    "policy_kwargs = dict(net_arch=[10,10])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1, gamma=1.0, tensorboard_log=\"./logs/\")\n",
    "total_timesteps = 200000\n",
    "model.learn(total_timesteps=total_timesteps,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cc66d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "model = DQN.load(\"./logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a628b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимальный агент политики по Авалленду\n",
    "\n",
    "def spread_func(beta, sigma, k):\n",
    "    return lambda T_t: spread(beta, sigma, T_t, k) \n",
    "\n",
    "def r_func(sigma, beta):\n",
    "    return lambda T_t, s, q: r(beta, sigma, T_t, s, q)\n",
    "    \n",
    "class OptimalAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        self.r_func = r_func(sigma, beta)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        spread = self.spread_func(observation[2])\n",
    "        r_ = self.r_func(observation[2], observation[0], observation[1])\n",
    "        \n",
    "        bid = r_ - spread/2\n",
    "        ask = r_ + spread/2\n",
    "\n",
    "        ds = observation[0] - r_\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#agent = OptimalAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ad49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# симметричный агент политики согласно Авалленду\n",
    "\n",
    "class SymmetricAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        #spread = self.spread_func(observation[2])\n",
    "        return 0\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#symmetric_agent = SymmetricAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce61c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env_agent_comp(envs, agent_rl,agent_opt,agent_sym):\n",
    "    \n",
    "    env = envs[0]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_rl = np.zeros(env.n)\n",
    "    asks_rl = np.zeros(env.n)\n",
    "    ss_rl = np.zeros(env.n)\n",
    "    ws_rl = np.zeros(env.n)\n",
    "    qs_rl = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_rl = 0.0\n",
    "    while not final:\n",
    "\n",
    "        action_rl = agent_rl.predict(obs,deterministic=True)\n",
    "        ss_rl[i] = obs[0]\n",
    "        qs_rl[i] = obs[1]\n",
    "        \n",
    "        despl = (action_rl[0]-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        ba_spread = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "\n",
    "        bids_rl[i] = ss_rl[i] - despl - ba_spread/2\n",
    "        asks_rl[i] = ss_rl[i] - despl + ba_spread/2\n",
    "\n",
    "        obs, reward, final, w_rl = env.step(action_rl[0])\n",
    "        i += 1\n",
    "        total_reward_rl += reward\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "    env = envs[1]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_opt = np.zeros(env.n)\n",
    "    asks_opt = np.zeros(env.n)\n",
    "    ds_opt = np.zeros(env.n)\n",
    "    spread_opt = np.zeros(env.n)\n",
    "    ss_opt = np.zeros(env.n)\n",
    "    ws_opt = np.zeros(env.n)\n",
    "    qs_opt = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_opt = 0.0\n",
    "    while not final:\n",
    "        action_opt = agent_opt.step(obs)\n",
    "\n",
    "        ds_opt[i] = action_opt\n",
    "        spread_opt[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_opt[i] = obs[0]\n",
    "        qs_opt[i] = obs[1]\n",
    "\n",
    "        bids_opt[i] = ss_opt[i] - ds_opt[i] - spread_opt[i]/2\n",
    "        asks_opt[i] = ss_opt[i] - ds_opt[i] + spread_opt[i]/2\n",
    "\n",
    "        obs, reward, final, w_opt = env.step(action_opt)\n",
    "        total_reward_opt += reward\n",
    "        i += 1\n",
    "\n",
    "    env = envs[2]\n",
    "\n",
    "    obs = env.reset()\n",
    "    bids_sym = np.zeros(env.n)\n",
    "    asks_sym = np.zeros(env.n)\n",
    "    ds_sym = np.zeros(env.n)\n",
    "    spread_sym = np.zeros(env.n)\n",
    "    ss_sym = np.zeros(env.n)\n",
    "    ws_sym = np.zeros(env.n)\n",
    "    qs_sym = np.zeros(env.n)    \n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_sym = 0.0\n",
    "    while not final:\n",
    "        action_sym = agent_sym.step(obs)\n",
    "\n",
    "        ds_sym[i] = action_sym\n",
    "        spread_sym[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_sym[i] = obs[0]\n",
    "        qs_sym[i] = obs[1]\n",
    "\n",
    "        bids_sym[i] = ss_sym[i] - ds_sym[i] - spread_sym[i]/2\n",
    "        asks_sym[i] = ss_sym[i] - ds_sym[i] + spread_sym[i]/2\n",
    "        \n",
    "        obs, reward, final, w_sym = env.step(action_sym)\n",
    "        i += 1\n",
    "        total_reward_sym += reward\n",
    "\n",
    "        \n",
    "    return w_rl['w'], w_opt['w'], w_sym['w'],total_reward_rl,total_reward_opt,total_reward_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecdc5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "1.0%\n",
      "2.0%\n",
      "3.0%\n",
      "4.0%\n",
      "5.0%\n",
      "6.0%\n",
      "7.0%\n",
      "8.0%\n",
      "9.0%\n",
      "10.0%\n",
      "11.0%\n",
      "12.0%\n",
      "13.0%\n",
      "14.0%\n",
      "15.0%\n",
      "16.0%\n",
      "17.0%\n",
      "18.0%\n",
      "19.0%\n",
      "20.0%\n",
      "21.0%\n",
      "22.0%\n",
      "23.0%\n",
      "24.0%\n",
      "25.0%\n",
      "26.0%\n",
      "27.0%\n",
      "28.0%\n",
      "29.0%\n",
      "30.0%\n",
      "31.0%\n",
      "32.0%\n",
      "33.0%\n",
      "34.0%\n",
      "35.0%\n",
      "36.0%\n",
      "37.0%\n",
      "38.0%\n",
      "39.0%\n",
      "40.0%\n",
      "41.0%\n",
      "42.0%\n",
      "43.0%\n",
      "44.0%\n",
      "45.0%\n",
      "46.0%\n",
      "47.0%\n",
      "48.0%\n",
      "49.0%\n",
      "50.0%\n",
      "51.0%\n",
      "52.0%\n",
      "53.0%\n",
      "54.0%\n",
      "55.0%\n",
      "56.0%\n",
      "57.0%\n",
      "58.0%\n",
      "59.0%\n",
      "60.0%\n",
      "61.0%\n",
      "62.0%\n",
      "63.0%\n",
      "64.0%\n",
      "65.0%\n",
      "66.0%\n",
      "67.0%\n",
      "68.0%\n",
      "69.0%\n",
      "70.0%\n",
      "71.0%\n",
      "72.0%\n",
      "73.0%\n",
      "74.0%\n",
      "75.0%\n",
      "76.0%\n",
      "77.0%\n",
      "78.0%\n",
      "79.0%\n",
      "80.0%\n",
      "81.0%\n",
      "82.0%\n",
      "83.0%\n",
      "84.0%\n",
      "85.0%\n",
      "86.0%\n",
      "87.0%\n",
      "88.0%\n",
      "89.0%\n",
      "90.0%\n",
      "91.0%\n",
      "92.0%\n",
      "93.0%\n",
      "94.0%\n",
      "95.0%\n",
      "96.0%\n",
      "97.0%\n",
      "98.0%\n",
      "99.0%\n"
     ]
    }
   ],
   "source": [
    "number_of_sims = 1000\n",
    "\n",
    "n = int(T/dt)\n",
    "ws_rl = np.zeros(number_of_sims)\n",
    "ws_opt = np.zeros(number_of_sims)\n",
    "ws_sym = np.zeros(number_of_sims)\n",
    "tr_rl = np.zeros(number_of_sims)\n",
    "tr_opt = np.zeros(number_of_sims)\n",
    "tr_sym = np.zeros(number_of_sims)\n",
    "\n",
    "envs = [Environment(s0, T, dt, sigma, beta, k, A, kappa),Environment(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False),Environment(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False)]\n",
    "for i in range(number_of_sims):\n",
    "    if i%10 == 0:\n",
    "        print(str(i/10) + \"%\")\n",
    "    ws_rl[i], ws_opt[i], ws_sym[i], tr_rl[i], tr_opt[i], tr_sym[i] = run_env_agent_comp(envs, model,OptimalAgent(beta, sigma, k),SymmetricAgent(beta, sigma, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a26a4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Гистограмма накопленного Зароботка')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure 2 (p. 222)\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([ws_opt,ws_rl], bins=30,edgecolor='black', label=['Оптимальное','Дискретное RL'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Зароботок\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного Зароботка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf70d308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Гистограмма накопленного зароботка')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_sym,tr_rl], bins=30,edgecolor='black', label=['Оптимальное', 'Симметричное','Дискретное обучение с подкреплением'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Общая награда\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного зароботка\")\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_rl], bins=30,edgecolor='black', label=['Оптимальное','Дискретное обучение с подкреплением'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Общая награда\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.title(\"Гистограмма накопленного зароботка\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a02785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное:\n",
      "47.79761695404737\n",
      "6.099978058681764\n",
      "7.83570309503321\n",
      "Симетричное:\n",
      "57.676984427753716\n",
      "11.86348257940186\n",
      "4.8617245435076715\n",
      "RL:\n",
      "52.49638481976592\n",
      "9.149381484479033\n",
      "5.737697669380224\n",
      "\n",
      "Оптимальное значение функции полезности: \t-2.6283680458275698e-09\n",
      "Значение симметричной функции полезности: \t-4.34167929750502e-06\n",
      "Значение функции полезности RL: \t\t-1808013474949.1975\n"
     ]
    }
   ],
   "source": [
    "print(\"Оптимальное:\")\n",
    "print(np.mean(ws_opt))\n",
    "print(np.std(ws_opt))\n",
    "print(np.mean(ws_opt)/np.std(ws_opt))\n",
    "print(\"Симетричное:\")\n",
    "print(np.mean(ws_sym))\n",
    "print(np.std(ws_sym))\n",
    "print(np.mean(ws_sym)/np.std(ws_sym))\n",
    "print(\"RL:\")\n",
    "print(np.mean(ws_rl))\n",
    "print(np.std(ws_rl))\n",
    "print(np.mean(ws_rl)/np.std(ws_rl))\n",
    "\n",
    "print()\n",
    "\n",
    "utility_avellaneda = np.mean(-np.exp(-beta*ws_opt))\n",
    "utility_rl = np.mean(-np.exp(-beta*ws_rl))\n",
    "\n",
    "print(\"Оптимальное значение функции полезности: \\t{}\".format(utility_avellaneda))\n",
    "print(\"Значение симметричной функции полезности: \\t{}\".format(np.mean(-np.exp(-beta*ws_sym))))\n",
    "print(\"Значение функции полезности RL: \\t\\t{}\".format(utility_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ac6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное:\n",
      "22.46050972128678\n",
      "3.4646472377108304\n",
      "6.482769580930536\n",
      "Симетричное:\n",
      "-7.172303914746957\n",
      "42.1491264960118\n",
      "-0.1701649479123988\n",
      "RL:\n",
      "26.78896490357231\n",
      "18.108305840467747\n",
      "1.4793744450519142\n"
     ]
    }
   ],
   "source": [
    "print(\"Оптимальное:\")\n",
    "print(np.mean(tr_opt))\n",
    "print(np.std(tr_opt))\n",
    "print(np.mean(tr_opt)/np.std(tr_opt))\n",
    "\n",
    "print(\"Симетричное:\")\n",
    "print(np.mean(tr_sym))\n",
    "print(np.std(tr_sym))\n",
    "print(np.mean(tr_sym)/np.std(tr_sym))\n",
    "\n",
    "print(\"RL:\")\n",
    "print(np.mean(tr_rl))\n",
    "print(np.std(tr_rl))\n",
    "print(np.mean(tr_rl)/np.std(tr_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5483a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_ws.csv', 'w') as f: \n",
    "      \n",
    "    # используя метод csv.writer из пакета CSV\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(ws_rl):\n",
    "        write.writerow([ws_rl[index], ws_opt[index],ws_sym[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5219fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_tr.csv', 'w') as f: \n",
    "      \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(tr_rl):\n",
    "        write.writerow([tr_rl[index], tr_opt[index],tr_sym[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d7bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
